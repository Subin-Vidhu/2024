{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "In this section, we'll be learning about different cross-validation techniques you can use for ML models. We'll go over these cross validation techniques by analyzing a fraud detection dataset. At large tech companies, fraud becomes an important problem that directly affects the company's bottom line. For example, Uber had a HUGE fraud problem especially when they expanded into international markets. \n",
    "\n",
    "In this notebook, we'll be covering:\n",
    "\n",
    "- Train-Test-Split\n",
    "\n",
    "- Leave-One-Out Cross Validation\n",
    "\n",
    "- K-Fold Cross Validation\n",
    "\n",
    "- Date Split\n",
    "\n",
    "- Time Series Split\n",
    "\n",
    "- Expanding Window\n",
    "\n",
    "- Monte Carlo Cross Validation\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "First, we'll import the standard python libraries we commonly use for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Next, we'll load our fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fraud_data.csv\")\n",
    "\n",
    "## Sample down to improve speed\n",
    "pos = df[df['isFraud'] == 1].copy()\n",
    "neg = df[df['isFraud'] == 0].sample(100000)\n",
    "\n",
    "df = pos.append(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split\n",
    "\n",
    "First method we'll go over is train-test-split. Train-test-split is the simplest form of cross-validation. We simply randomly slice our dataset into a training set and testing set. Typically, the most important parameters are: \n",
    "\n",
    "`X`: The feature set you're looking to split. \n",
    "\n",
    "`y`: The target variable you're looking to split.\n",
    "\n",
    "`test_size`: The size of your testing set. Typically, this is denoted as a fraction such as `0.33`. \n",
    "\n",
    "`random_state`: This is the seed of the random shuffle. I recommend setting a seed so everytime you rerun your notebook, your results stay consistent. \n",
    "\n",
    "`stratify`: This is an optional argument. But stratifying will reduce the variance in the random shuffle to ensure that your training and testing sets are more similar than not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "features = [\n",
    "    'amount',\n",
    "    'oldbalanceOrg',\n",
    "    'newbalanceOrig',\n",
    "    'oldbalanceDest',\n",
    "    'newbalanceDest'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['isFraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "print(average_precision_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "The next common method of cross validation is K-Fold. To review, K-Fold is we’re essentially dividing our dataset into multiple datasets, then running train-test-split multiple times, across these subsets.\n",
    "\n",
    "Import parameters we should keep in mind: \n",
    "\n",
    "`n_splits`: This is the number of splits we want to make within our dataset. \n",
    "\n",
    "`shuffle`: This tells us whether we should shuffle our data before splitting into folds. \n",
    "\n",
    "`random_state`: This is the random seed we're setting, similar to train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "folds = {}\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    # Fold\n",
    "    fold_number = 1\n",
    "    # Store fold number\n",
    "    folds[fold_number] = (df.iloc[train], df.iloc[test])\n",
    "    print('train: %s, test: %s' % (df.iloc[train], df.iloc[test]))\n",
    "    fold_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, after completing K-Fold Cross-Validation we'll want to calculate a cross-validation score. Typically, we'll get the scores for each fold, then take an average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=kf, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation\n",
    "\n",
    "Another type of cross-validation is Leave One Out Cross-Validation. The idea here is that we’re training our model on all the data, then leaving one data point out, and evaluating our model on a single data point. We do this for every datapoint in the entire dataset. For the sake of time, we're goint to limit our dataset to 100 data points here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for train_index, test_index in loo.split(X[:100]):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    correct = y_preds[0] == y_test.values[0]\n",
    "    \n",
    "    all_preds.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(all_preds)/len(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split Date Split\n",
    "\n",
    "In many instances, you don't want to randomly slice your data into training and testing sets, but instead, you want to split it by time. In this case, you'll want to split by date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923704488908898\n"
     ]
    }
   ],
   "source": [
    "DATE = '2021-12-31'\n",
    "\n",
    "train_df = df[df['date'] < DATE].copy()\n",
    "test_df = df[df['date'] >= DATE].copy()\n",
    "\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "\n",
    "y_train = train_df['isFraud']\n",
    "y_test = test_df['isFraud']\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "print(average_precision_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window/Time Series KFold\n",
    "\n",
    "The problem with splitting by date is that the resulting training and testing sets can vary, depending on the date you select. The solution to this is to use a Time Series K-Fold Split. In time series Kfold, we're combining the elements of KFold and train-test date split. In Time Series Kfold, we'll be splitting our dataset multiple times, using differing dates. The size of the training sets will be the same for each fold: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.012475741613529248, 0.0068755198225672306, 0.005212087607429998, 0.004214028278347657, 0.0032159689492653174]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    pr_auc = average_precision_score(y_preds, y_test)\n",
    "    \n",
    "    all_scores.append(pr_auc)\n",
    "    \n",
    "    \n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Window\n",
    "\n",
    "One of the problems with standard time series split, is the size of the training sets stay the same. The solution to this is to use an expanding window split. The idea here, is with each incremental date split, we're using **all** the data rather than sliding the window. You can adapt the sci-kit learn version of TimeSeriesSplit. We also wrote a simple implementation as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandingWindowCV:\n",
    "    def fit(self, date_col, date_range = None, custom_range = None):\n",
    "        self.date_col = date_col\n",
    "        self.date_range = date_range\n",
    "        self.custom_range = custom_range\n",
    "        \n",
    "        if date_range is not None and custom_range is not None:\n",
    "            raise ValueError(\"Date Range and Custom Range both cannot be None.\")\n",
    "    \n",
    "    def split(self, df):\n",
    "        if self.date_range is None:         \n",
    "            dates = list(set(df[self.date_col].astype(str).values))\n",
    "        \n",
    "        if self.date_range is not None:\n",
    "            dates = pd.date_range(start=self.date_range[0], end=self.date_range[1])\n",
    "            dates = [str(d.date()) for d in dates]\n",
    "        \n",
    "        if self.custom_range is not None:\n",
    "            dates = self.custom_range\n",
    "            \n",
    "        for d in dates:\n",
    "            df_train = df[df[self.date_col].astype(str) <= d].copy()\n",
    "            df_test = df[df[self.date_col].astype(str) > d].copy()\n",
    "            yield df_train, df_test\n",
    "            \n",
    "ew = ExpandingWindowCV()\n",
    "ew.fit(date_col = 'date', date_range = ['2022-01-02','2022-01-08'])\n",
    "ew.split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9220560208631237,\n",
       " 0.9226582019111925,\n",
       " 0.9243608853720093,\n",
       " 0.9229844807192916,\n",
       " 0.9278032540111424,\n",
       " 0.921955918184176,\n",
       " 0.9253343888992496]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores = []\n",
    "\n",
    "for train_df, test_df in ew.split(df):\n",
    "    X_train = train_df[features]\n",
    "    X_test = test_df[features]\n",
    "\n",
    "    y_train = train_df['isFraud']\n",
    "    y_test = test_df['isFraud']\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    pr_auc = average_precision_score(y_preds, y_test)\n",
    "    \n",
    "    all_scores.append(pr_auc)\n",
    "    \n",
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Cross Validation\n",
    "\n",
    "The last method we'll go over is Monte Carlo Cross Validation. Monte Carlos Cross Validation is where we randomly select a sub-sample (with replacement) from our dataset for the training set, use the rest for the testing set. Repeat this (with replacement) N number of times, to create a distribution of evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "rs.get_n_splits(df)\n",
    "\n",
    "all_scores = []\n",
    "for train_index, test_index in rs.split(df):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    pr_auc = average_precision_score(y_preds, y_test)\n",
    "    \n",
    "    all_scores.append(pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9340994244507013,\n",
       " 0.9356447139288409,\n",
       " 0.9230229692911995,\n",
       " 0.9411872091474103,\n",
       " 0.9278329879205108]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you learned about a variety of cross validation techniques: \n",
    "\n",
    "- Train-Test-Split\n",
    "\n",
    "- Leave-One-Out Cross Validation\n",
    "\n",
    "- K-Fold Cross Validation\n",
    "\n",
    "- Date Split\n",
    "\n",
    "- Time Series Split\n",
    "\n",
    "- Expanding Window\n",
    "\n",
    "- Monte Carlo Cross Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
