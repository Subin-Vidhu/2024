{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "In this section, we'll be learning different missing value imputation techniques by analyzing simulated customer lifetime value data. Customer Lifetime Value is the total monetary worth the customer has to the business, over the course of its business-customer relationship. In this dataset, to keep things simple, we'll be using purchases as a proxy for CLV. \n",
    "\n",
    "We'll be covering how to:\n",
    "\n",
    "- Check the number of null values\n",
    "\n",
    "- Dropping Null Values\n",
    "\n",
    "- Mean/Median/Mode Imputation\n",
    "\n",
    "- Multiple Imputation using Regression\n",
    "\n",
    "- Imputation using Nearest Neighbors\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "First, we'll need to import the relevant libraries. We'll be using the standard `pandas`, `numpy` libraries for data manipulation. We'll then be using `sklearn` for the more advanced imputation techniques. We will use `scipy` for the `mode` imputation later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Next, we'll load our customer lifetime value dataset. You'll see in our dataset, we have about 6 columns. The `purchases` column is the column we care about in our customer lifetime value problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df = pd.read_csv(\"clv_data.csv\")\n",
    "\n",
    "df['lifetime_value'] = df['purchases'] * 20\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(5,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Null Values\n",
    "\n",
    "The first step in any data analysis or ML model is to check null values. We can check the number of nulls in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you want to see the percentages, we wrote a function you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_summary_table(df):\n",
    "    \"\"\"\n",
    "    Returns a summary table showing null value counts and percentage\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataframe to check\n",
    "    \n",
    "    Returns:\n",
    "    null_values (DataFrame)\n",
    "    \"\"\"\n",
    "    null_values = pd.DataFrame(df.isnull().sum())\n",
    "    null_values[1] = null_values[0]/len(df)\n",
    "    null_values.columns = ['null_count','null_pct']\n",
    "    return null_values\n",
    "\n",
    "nulls_summary_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Null Values\n",
    "\n",
    "Dropping nulls is the quickest and easiest method to dropping nulls. We will use the internal pandas method `dropna` which will simply drop all rows that contain nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = df.copy()\n",
    "\n",
    "drop_df = drop_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d = drop_df[['age','days_on_platform','income']]\n",
    "y_d = drop_df['lifetime_value']\n",
    "\n",
    "\n",
    "X_train_d = X_d[:4000]\n",
    "y_train_d = y_d[:4000]\n",
    "\n",
    "X_test_d = X_d[1000:]\n",
    "y_test_d = y_d[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean/Median/Mode Imputation\n",
    "\n",
    "The next is mean/median/mode imputation. We can use the native numpy functions for the mean and median. We can use scipy for the mode. Then, pandas as a native `fillna` method we can use to impute the nulls with the mean/median/mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df.copy()\n",
    "\n",
    "X_m = m_df[['age','days_on_platform','income']]\n",
    "y_m = m_df['lifetime_value']\n",
    "\n",
    "\n",
    "X_train_m = X_m[:4000]\n",
    "y_train_m = y_m[:4000]\n",
    "\n",
    "X_test_m = X_m[1000:]\n",
    "y_test_m = y_m[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean\n",
    "X_train_m.loc[:,'age'] = X_train_m['age'].fillna(np.mean(X_train_m['age']))\n",
    "X_test_m.loc[:,'age'] = X_test_m['age'].fillna(np.mean(X_train_m['age'])) ## Cannot use training dataset to impute\n",
    "\n",
    "\n",
    "X_train_m.loc[:,'days_on_platform'] = X_train_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform']))\n",
    "X_test_m.loc[:,'days_on_platform'] = X_test_m['days_on_platform'].fillna(np.mean(X_train_m['days_on_platform'])) ## Cannot use training dataset to impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Median\n",
    "m_df.loc[:,'age'] = df['age'].fillna(np.median(m_df['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mode\n",
    "m_df.loc[:,'age'] = m_df['age'].fillna(stats.mode(m_df['age'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Imputation Using Regression\n",
    "\n",
    "Now that we've covered the simpler imputation techniques, we'll cover a more complicated imputation technique: Multiple Imputation. Multiple Imputation requires you to have some knowledge of ML modeling because we are using an ML model to impute the missing values. We won't go over every argument, but we'll go over the key ones. \n",
    "\n",
    "Multiple imputation has a few different estimators, using the `estimator` argument:\n",
    "\n",
    "- `BayesianRidge`: Regularized Linear Regression\n",
    "\n",
    "- `RandomForestRegressor`: Random Forest Model. Mimics missForest in the R language.\n",
    "\n",
    "We'll go over how these estimators work in the next course: ML Algorithms. \n",
    "\n",
    "The `missing_values` argument is a placeholder for the data type of the missing values you want to impute. \n",
    "\n",
    "It's important to use `add_indicatorbool` as it'll create a placeholder indicating that you've imputed a missing value. This is important, because there could be patterns behind how a value is missing. Adding an indicator would allow you to keep track of where you made an imputation. Plus, it could also add signal into your model. \n",
    "\n",
    "`max_iter`: The number of iteration rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "## Target - Purchases in the first six months\n",
    "\n",
    "r_df = df.copy()\n",
    "\n",
    "X_r = r_df[['age','days_on_platform','income']]\n",
    "y_r = r_df['lifetime_value']\n",
    "\n",
    "\n",
    "X_train_r = X_r[:4000]\n",
    "y_train_r = y_r[:4000]\n",
    "\n",
    "X_test_r = X_r[1000:]\n",
    "y_test_r = y_r[1000:]\n",
    "\n",
    "\n",
    "Imp = IterativeImputer(max_iter=10, random_state = 0)\n",
    "Imp.fit(X_train_r)\n",
    "\n",
    "X_train_r = Imp.transform(X_train_r)\n",
    "X_test_r = Imp.transform(X_test_r)\n",
    "\n",
    "X_train_r = pd.DataFrame(X_train_r)\n",
    "X_train_r.columns = X_train_r.columns\n",
    "\n",
    "X_test_r = pd.DataFrame(X_test_r)\n",
    "X_test_r.columns = X_test_r.columns\n",
    "\n",
    "r_df = pd.concat([X_train_r,X_test_r],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Imputation\n",
    "\n",
    "On top of using linear regression or random forest regression to impute values, you can also use nearest neighbors imputation. Nearest neighbor imputation essentially uses a K-Nearest Neighbors algorithm to find the most similar data points, to impute the null values. \n",
    "\n",
    "**Important Parameters**\n",
    "\n",
    "`missing_values`: This is the type of null value you want to impute. Typically, this is NaN, but it could be float or whichever you decide. \n",
    "\n",
    "`n_neighbors`: The number of neighbors to use for imputation. You can add more or less. Fewer neighbors can lead to overfitting. Larger numbers will lose some precision. \n",
    "\n",
    "`weights`: Pick how you want to weight all points in each neighborhood. There are two typical ways: `'uniform'` or `'distance'`. Uniform is equal weighting. Distance is weighted by the distance from the point. \n",
    "\n",
    "- `callable` : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "`metric`: The distance metric used to search for neighbors. The default is euclidean.\n",
    "\n",
    "`add_indicator`: This will add a dummy feature 0 or 1 if the value was imputed, similar to `add_indicatorbool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "imputer.fit(X_train_r)\n",
    "X_train_k = imputer.transform(X_train_r)\n",
    "X_test_k = imputer.transform(X_test_r)\n",
    "\n",
    "y_train_k = y_train_r.copy()\n",
    "y_test_k = y_test_r.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Drop Null Model\n",
    "clf_n = RandomForestRegressor(random_state=0)\n",
    "clf_n.fit(X_train_d, y_train_d)\n",
    "pred_dropna = clf_n.predict(X_test_d)\n",
    "\n",
    "# Mean Imputation Model\n",
    "clf_m = RandomForestRegressor(random_state=0)\n",
    "clf_m.fit(X_train_m, y_train_m)\n",
    "pred_m = clf_m.predict(X_test_m)\n",
    "\n",
    "# Regression Imputation\n",
    "clf_r = RandomForestRegressor(random_state=0)\n",
    "clf_r.fit(X_train_r, y_train_r)\n",
    "pred_r = clf_r.predict(X_test_r)\n",
    "\n",
    "#Nearest Neighbor Imputation\n",
    "clf_n = RandomForestRegressor(random_state=0)\n",
    "clf_n.fit(X_train_k, y_train_k)\n",
    "pred_k = clf_n.predict(X_test_k)\n",
    "\n",
    "\n",
    "print('Drop Null MAE Score: %.3f' % mean_absolute_error(y_test_d,pred_dropna))\n",
    "print('Mean Impute MAE Score: %.3f' % mean_absolute_error(y_test_m,pred_m))\n",
    "print('Regression MAE Score: %.3f '% mean_absolute_error(y_test_r,pred_r))\n",
    "print('Nearest Neighbor MAE Score: %.3f'% mean_absolute_error(y_test_k,pred_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you learned about a variety of missing value imputation techniques, ranging from simple to more complex. We went over:\n",
    "\n",
    "- Dropping Nulls\n",
    "\n",
    "- Mean/Median/Mode Imputation\n",
    "\n",
    "- Regression Imputation (linear and forests)\n",
    "\n",
    "- Nearest Neighbor Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
