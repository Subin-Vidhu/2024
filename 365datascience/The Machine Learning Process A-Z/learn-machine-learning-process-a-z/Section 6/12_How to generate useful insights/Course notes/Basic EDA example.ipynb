{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T20:16:55.339467Z","iopub.execute_input":"2022-08-31T20:16:55.339797Z","iopub.status.idle":"2022-08-31T20:16:55.348326Z","shell.execute_reply.started":"2022-08-31T20:16:55.339763Z","shell.execute_reply":"2022-08-31T20:16:55.347596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis for \"The Machine Learning Process Course on 365 Data Science\". \nWe are analyzing real data from my YouTube channel. The focus of EDA is to understand patterns in the data so that we can start generating insights and predictions. In this notebook, I walk through the main EDA concepts from the machine learning process course. We cover:\n- Single Variable Plots\n - Histograms \n - Box Plots\n - Bar charts\n- Relationships & Multi-variable plots \n - Scatterplots\n - Correlation Matrices \n - Pivot Tables \n - Bar Charts \n - Line Charts\n \n \n I will be using matplotlib and Seaborn to visualize this data. It should be noted that there are plenty of different visualization libraries to choose from. I personally use plotly quite a lot in my own personal projects. I find that these two that we are using have good basics that are easy to understand and build on. ","metadata":{}},{"cell_type":"markdown","source":"## Loading in the data\nOur first step is to load in the data and get a feel for what we will be working with. We do the following steps before we start with the true EDA:\n1) Load in our libraries that we plan to use for data manipulation and visualization \n\n2) Load in our data \n\n3) Explore the high level features of our data (size, columns, etc.)\n\n4) Additional cleaning of our data if needed \n\n5) Explore high level descriptive statistics of our data","metadata":{}},{"cell_type":"code","source":"#import basic visualization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns  \n#clean columns ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:55.854673Z","iopub.execute_input":"2022-08-31T20:16:55.855804Z","iopub.status.idle":"2022-08-31T20:16:57.024586Z","shell.execute_reply.started":"2022-08-31T20:16:55.855755Z","shell.execute_reply":"2022-08-31T20:16:57.023546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read in data \ndf_agg = pd.read_csv('/kaggle/input/ken-jee-youtube-data/Aggregated_Metrics_By_Video.csv',encoding='utf-8')\ndf_agg_country_sub = pd.read_csv('/kaggle/input/ken-jee-youtube-data/Aggregated_Metrics_By_Country_And_Subscriber_Status.csv', encoding='utf-8')\ndf_ts = pd.read_csv('/kaggle/input/ken-jee-youtube-data/Video_Performance_Over_Time.csv', encoding='utf-8')\ndf_comments = pd.read_csv('/kaggle/input/ken-jee-youtube-data/All_Comments_Final.csv', encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.026746Z","iopub.execute_input":"2022-08-31T20:16:57.027248Z","iopub.status.idle":"2022-08-31T20:16:57.861017Z","shell.execute_reply.started":"2022-08-31T20:16:57.027207Z","shell.execute_reply":"2022-08-31T20:16:57.860096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at columns for each dataframe \nprint(df_agg.columns)\nprint(df_agg_country_sub.columns)\nprint(df_ts.columns)\nprint(df_comments.columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.862935Z","iopub.execute_input":"2022-08-31T20:16:57.863196Z","iopub.status.idle":"2022-08-31T20:16:57.870240Z","shell.execute_reply.started":"2022-08-31T20:16:57.863165Z","shell.execute_reply":"2022-08-31T20:16:57.869548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the column headers have some extra non-ascii characters, we need to clean them up before we do our analysis\n#this goes through each column and removes all the non-ascii characters \n\nnewcols =[x.encode(\"ascii\", \"ignore\").decode('utf-8') for x in df_agg.columns]\ndf_agg.columns = newcols","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.872149Z","iopub.execute_input":"2022-08-31T20:16:57.873064Z","iopub.status.idle":"2022-08-31T20:16:57.887802Z","shell.execute_reply.started":"2022-08-31T20:16:57.873018Z","shell.execute_reply":"2022-08-31T20:16:57.886488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_agg.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.889633Z","iopub.execute_input":"2022-08-31T20:16:57.890078Z","iopub.status.idle":"2022-08-31T20:16:57.904092Z","shell.execute_reply.started":"2022-08-31T20:16:57.889962Z","shell.execute_reply":"2022-08-31T20:16:57.903321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using .describe() gives us basic descriptive statistics for all of our numeric data. From this we can see which variables we might want to explore more. \n#We can see things like: which variables might have outliers, which might have skew, or which have a wide range of values\ndf_agg.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.905244Z","iopub.execute_input":"2022-08-31T20:16:57.905627Z","iopub.status.idle":"2022-08-31T20:16:57.985447Z","shell.execute_reply.started":"2022-08-31T20:16:57.905595Z","shell.execute_reply":"2022-08-31T20:16:57.984486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single variable plots\nAfter looking at the descriptive statistics, we may want to explore our numeric features more. To do this, we like to use single variable plots. These can help us understand the distributions of our data. If our data doesn't follow a normal distribution, we may want to make some transform it so it can be used by specific types of models like linear regresison. \n\nLooking at these charts can also help us to evaluate if there are outliers present in our data. \n\nFirst, we will look at histograms of our data. These help us to see skew as well as some outliers. I usually do this with every feature (if possible). Try more of these on your own! \n\nNext, we will explore some box plots to see if they tell us additional information\n\nFinally, we will explore some distribution plots for categorical variables.","metadata":{}},{"cell_type":"code","source":"#you can easily see the distribuiton of data in a column with panda's built in .hist() method. Here is the distribution for likes. There is clearly one sample that has way more like sthan the others, this is something we could look into more\n\ndf_agg.Likes.hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:57.986943Z","iopub.execute_input":"2022-08-31T20:16:57.987821Z","iopub.status.idle":"2022-08-31T20:16:58.449993Z","shell.execute_reply.started":"2022-08-31T20:16:57.987780Z","shell.execute_reply":"2022-08-31T20:16:58.449218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's look at the same thing for average percentage viewed. This will have no outliers unlike the likes column. We are using matplotlib's hist function here instead of the integration through pandas. \nplt.hist(df_agg['Average percentage viewed (%)'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:58.451185Z","iopub.execute_input":"2022-08-31T20:16:58.451424Z","iopub.status.idle":"2022-08-31T20:16:58.684857Z","shell.execute_reply.started":"2022-08-31T20:16:58.451382Z","shell.execute_reply":"2022-08-31T20:16:58.683904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this data has a significant amount of right skew, we may want to transform this data if we were planning to use linear regression.\ndf_agg['Impressions click-through rate (%)'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:58.686431Z","iopub.execute_input":"2022-08-31T20:16:58.686659Z","iopub.status.idle":"2022-08-31T20:16:58.936008Z","shell.execute_reply.started":"2022-08-31T20:16:58.686631Z","shell.execute_reply":"2022-08-31T20:16:58.934821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From this boxplot, we can see that there are quite a few outliers from our normal data. What does this tell us about the nature of likes on videos? Perhaps we shouldn't be trusting averages if something can skew so high. \nplt.boxplot(df_agg['Likes'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:58.938572Z","iopub.execute_input":"2022-08-31T20:16:58.938818Z","iopub.status.idle":"2022-08-31T20:16:59.057465Z","shell.execute_reply.started":"2022-08-31T20:16:58.938781Z","shell.execute_reply":"2022-08-31T20:16:59.056718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this plot looks a lot more normal. We can se emost of the videos are viewed between 25-45% of the way through. We still have some outliers, what may be special or different about those? \nplt.boxplot(df_agg['Average percentage viewed (%)'])\n#","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:59.059000Z","iopub.execute_input":"2022-08-31T20:16:59.059535Z","iopub.status.idle":"2022-08-31T20:16:59.269890Z","shell.execute_reply.started":"2022-08-31T20:16:59.059480Z","shell.execute_reply":"2022-08-31T20:16:59.269165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with click through rate, we also see quite a few high outliers versus the median of / interquartile range.\nplt.boxplot(df_agg['Impressions click-through rate (%)'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:59.271502Z","iopub.execute_input":"2022-08-31T20:16:59.272079Z","iopub.status.idle":"2022-08-31T20:16:59.466033Z","shell.execute_reply.started":"2022-08-31T20:16:59.272018Z","shell.execute_reply":"2022-08-31T20:16:59.465321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To show how to do this with categorical data, let's make a categorical column.\n#We can take our revenue data and make it into different categories that are relevant to me. Usually I'm interested if videos have made less than $100, \n#between $100-1000 and over $1000. \n\n#Let's quickly \"engineer these categories for ourself\"\n\n#make bins from 0-100, 100-1000, and greater than 1000\nbins = pd.IntervalIndex.from_tuples([(0, 100), (100, 1000), (1000,float(\"inf\"))])\ndf_agg['rev_buckets'] = pd.cut(df_agg['Your estimated revenue (USD)'],bins)\n\n#get count of number of videos by reveune bucket\nrev_values = df_agg['rev_buckets'].value_counts()\nrev_values.plot.bar()\n\n#We can get a sense of the balance of categorical variables using a bar chart like this. ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:59.467331Z","iopub.execute_input":"2022-08-31T20:16:59.467789Z","iopub.status.idle":"2022-08-31T20:16:59.659467Z","shell.execute_reply.started":"2022-08-31T20:16:59.467741Z","shell.execute_reply":"2022-08-31T20:16:59.658337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Relationships and Multi-Variable Plots\nA big part of exploratory data analysis is seeing how mutliple variables are related. We can use multiple different types of plots to easily see these relationships. By understanding these relationships we can start to udnerstand which features may be releated or can serve to predict others. In my opinion, this is where buisiness value starts to emerge. \n\nIn this part we will explore:\n\n1) Scatter plots\n\n2) Correlation Matrices \n\n3) Pivot Tables\n\n4) Bar Charts\n\n5) Line Charts\n ","metadata":{}},{"cell_type":"markdown","source":"### Scatter Plots\n\nWe use these to see if there is a relationship between two datapoints. \n\nLet's take a look  at a few variables and see if they may be correlated. We will explore if the average percentage viewed of the video is related to the cost per milli on the video (the amount youtube makes for 1000 views)\n","metadata":{}},{"cell_type":"code","source":"#create plot with matplotlib\nplt.scatter(df_agg['Average percentage viewed (%)'] ,df_agg['CPM (USD)'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:59.661134Z","iopub.execute_input":"2022-08-31T20:16:59.661994Z","iopub.status.idle":"2022-08-31T20:16:59.856437Z","shell.execute_reply.started":"2022-08-31T20:16:59.661937Z","shell.execute_reply":"2022-08-31T20:16:59.855421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's do the same thing with seaborn so we can see a trendline. This will be easier for us to build multiple of these \nsns.regplot(x='Average percentage viewed (%)',y='CPM (USD)', data = df_agg)\n\n#as we can see there is a very little correlation between the two variables ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:16:59.858650Z","iopub.execute_input":"2022-08-31T20:16:59.858901Z","iopub.status.idle":"2022-08-31T20:17:00.178700Z","shell.execute_reply.started":"2022-08-31T20:16:59.858851Z","shell.execute_reply":"2022-08-31T20:17:00.177739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's try another. How do variables RPM and CPM match up. RPM is how much the youtuber makes for an add. Again CPM is how much youtube sells the ad for. \nsns.regplot(x='RPM (USD)',y='CPM (USD)', data = df_agg)\n\n# Here we can see a lot stronger relationship between the two rates. \n# I should probably look into that one value where YouTube is making a massive margin on my video. These are the types of insights you can find in these graphs\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:00.180125Z","iopub.execute_input":"2022-08-31T20:17:00.181205Z","iopub.status.idle":"2022-08-31T20:17:00.599844Z","shell.execute_reply.started":"2022-08-31T20:17:00.181148Z","shell.execute_reply":"2022-08-31T20:17:00.598694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrices \n\nScatter plots are great for comparing two variables. Often we have many different variables that we want to see relationships between. \n\n\nIn this case we would use a correlation matrix. Let's do a correlation matrix for all of our value in the dataset. \n","metadata":{}},{"cell_type":"code","source":"#first, we get the correlations between our datapoints. We can see all the relationships that will be polotted via the pearson correlation coefficient \ncorr = df_agg.corr()\n\nsns.heatmap(corr)\n\n#this looks pretty awful, let's improve it's look","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:00.602467Z","iopub.execute_input":"2022-08-31T20:17:00.602904Z","iopub.status.idle":"2022-08-31T20:17:01.046091Z","shell.execute_reply.started":"2022-08-31T20:17:00.602856Z","shell.execute_reply":"2022-08-31T20:17:01.045140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A better example (formatting used in below chart) - https://seaborn.pydata.org/examples/many_pairwise_correlations.html \n\nsns.set_theme(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df_agg.corr()\n\n# Generate a mask for the upper triangle (otherwise this looks like the square we had above and is redundant)\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure \nf, ax = plt.subplots(figsize=(15, 10))\n\n# Generate a custom diverging colormap (choose colors here)\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio \nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, annot_kws={\"fontsize\":8})\n\n#obviously many of thes variables are HIGHLY correlated. Something we may want to explore is why Average percentage viewed is negatively related to RPM","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:01.047343Z","iopub.execute_input":"2022-08-31T20:17:01.047646Z","iopub.status.idle":"2022-08-31T20:17:02.168695Z","shell.execute_reply.started":"2022-08-31T20:17:01.047610Z","shell.execute_reply":"2022-08-31T20:17:02.167955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Pivot Tables\nSometimes we want to cut the data and compare how variables stack up. For example we may want to see which countries have the highest view duration. We can do this easily with pivot tables. \n\nWe will use the df_agg_country_sub dataset to explore this.\n","metadata":{}},{"cell_type":"code","source":"#pivot table to explore values. A basic pivot table takes the average of these different categories we choose.\n\npd.pivot_table(df_agg_country_sub, index = 'Country Code', values = 'Average View Percentage')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:02.170415Z","iopub.execute_input":"2022-08-31T20:17:02.170700Z","iopub.status.idle":"2022-08-31T20:17:02.201301Z","shell.execute_reply.started":"2022-08-31T20:17:02.170664Z","shell.execute_reply":"2022-08-31T20:17:02.200446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this time, let's compare average view percentage by country code and subscriber status.\npd.pivot_table(df_agg_country_sub, index = 'Country Code', columns = 'Is Subscribed',values = 'Average View Percentage')\n\n#we could plot this to see which countries have the biggest difference in subscribed watch time for subscribed vs not subscribed viewers ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:02.205003Z","iopub.execute_input":"2022-08-31T20:17:02.205613Z","iopub.status.idle":"2022-08-31T20:17:02.242337Z","shell.execute_reply.started":"2022-08-31T20:17:02.205574Z","shell.execute_reply":"2022-08-31T20:17:02.241357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's plot a slightly simpler graph. Let's just look at if subscribers or non-subsribers watch my videos for longer\npd.pivot_table(df_agg_country_sub, index = 'Is Subscribed', values = 'Average View Percentage').plot.bar()\n\n#it appears that subscribers to my channel watch my videos for longer than non-subscribers. This is another way that we can use a bar chart to compare multiple features","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:02.244324Z","iopub.execute_input":"2022-08-31T20:17:02.244774Z","iopub.status.idle":"2022-08-31T20:17:02.433982Z","shell.execute_reply.started":"2022-08-31T20:17:02.244724Z","shell.execute_reply":"2022-08-31T20:17:02.433304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Line Charts\nOften, we want to see how variables change over time. Line charts are great for this. Time is an important feature in many models! \n\nWe will use the df_ts dataset for this analys.","metadata":{}},{"cell_type":"code","source":"#First we need to make sure our date field is in the date time format. We do this by converting our string to datetime\ndf_ts['Date'] = pd.to_datetime(df_ts['Date'])\n\n#let's look at user subscriptions removed and see if there is any trend there. We will also compare this with user likes removed to see if we can find anything interesting\n\n#first, we have to aggregate these by video. We do this with a pivot table.\nrm_x_date = pd.pivot_table(df_ts, index='Date',values = 'User Subscriptions Removed', aggfunc ='sum').reset_index()\n\n#next we visualize this data with seaborn \nsns.lineplot(data=rm_x_date,x='Date', y='User Subscriptions Removed')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:02.435527Z","iopub.execute_input":"2022-08-31T20:17:02.436085Z","iopub.status.idle":"2022-08-31T20:17:03.139938Z","shell.execute_reply.started":"2022-08-31T20:17:02.436040Z","shell.execute_reply":"2022-08-31T20:17:03.138455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now let's compare this with two other related metrics, video likes removed and dislikes\n\nlikes_rm_date = pd.pivot_table(df_ts, index='Date',values='Video Likes Removed', aggfunc = 'sum').reset_index()\ndislikes_date = pd.pivot_table(df_ts, index='Date',values='Video Dislikes Added', aggfunc = 'sum').reset_index()\n\nsns.lineplot(data=rm_x_date,x='Date', y='User Subscriptions Removed')\nsns.lineplot(data=likes_rm_date,x='Date', y='Video Likes Removed')\nsns.lineplot(data=dislikes_date,x='Date', y='Video Dislikes Added')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:03.141428Z","iopub.execute_input":"2022-08-31T20:17:03.142062Z","iopub.status.idle":"2022-08-31T20:17:03.816355Z","shell.execute_reply.started":"2022-08-31T20:17:03.142014Z","shell.execute_reply":"2022-08-31T20:17:03.815479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was a bit messy, let's compare it over months instead of days.\ndf_ts['Month_Year'] = df_ts['Date'].dt.to_period('M')\n\n#create new pivot tables \nrm_x_date = pd.pivot_table(df_ts, index='Month_Year',values = 'User Subscriptions Removed', aggfunc ='sum').reset_index()\nlikes_rm_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Likes Removed', aggfunc = 'sum').reset_index()\ndislikes_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Dislikes Added', aggfunc = 'sum').reset_index()\n\n#create 3 separate line plots with pandas built in visualization tool \nrm_x_date.plot(x='Month_Year',y='User Subscriptions Removed')\nlikes_rm_date.plot(x='Month_Year',y='Video Likes Removed')\ndislikes_date.plot(x='Month_Year',y='Video Dislikes Added')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:17:22.261610Z","iopub.execute_input":"2022-08-31T20:17:22.262255Z","iopub.status.idle":"2022-08-31T20:17:23.234394Z","shell.execute_reply.started":"2022-08-31T20:17:22.262208Z","shell.execute_reply":"2022-08-31T20:17:23.233477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was a bit messy, let's compare it over months instead of days. To get everything on one graph, we need to make it so every video has the same day.\n#Just different months and years \ndf_ts['Month_Year'] = df_ts['Date'].apply(lambda x: x.replace(day=1))\n\nrm_x_date = pd.pivot_table(df_ts, index='Month_Year',values = 'User Subscriptions Removed', aggfunc ='sum').reset_index()\nlikes_rm_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Likes Removed', aggfunc = 'sum').reset_index()\ndislikes_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Dislikes Added', aggfunc = 'sum').reset_index()\n\n#create 3 line plots with seaborn all on one graph \nsns.lineplot(data=rm_x_date,x='Month_Year', y='User Subscriptions Removed', label ='Subs Removed')\nsns.lineplot(data=likes_rm_date,x='Month_Year', y='Video Likes Removed', label = 'Likes Removed')\nsns.lineplot(data=dislikes_date,x='Month_Year', y='Video Dislikes Added', label = 'Dislikes')\n\n#it seems like there is a big spike in unsubscribes and negative comments during this period. We should explore this more.\n#maybe we should divide these all by views to determine if the spike is only related to a spike in viewership as well.\n#or maybe a single video caused a lot of negative things this period ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:22:45.628933Z","iopub.execute_input":"2022-08-31T20:22:45.629371Z","iopub.status.idle":"2022-08-31T20:22:47.287390Z","shell.execute_reply.started":"2022-08-31T20:22:45.629332Z","shell.execute_reply":"2022-08-31T20:22:47.286218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's normalize this by views \n\ndf_ts['Month_Year'] = df_ts['Date'].apply(lambda x: x.replace(day=1))\n\nmonthly_views = pd.pivot_table(df_ts, index = 'Month_Year', values = 'Views', aggfunc = 'sum')\nrm_x_date = pd.pivot_table(df_ts, index='Month_Year',values = 'User Subscriptions Removed', aggfunc ='sum') / monthly_views.values\nlikes_rm_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Likes Removed', aggfunc = 'sum') / monthly_views.values\ndislikes_date = pd.pivot_table(df_ts, index='Month_Year',values='Video Dislikes Added', aggfunc = 'sum') / monthly_views.values\n\n#create 3 line plots with seaborn all on one graph \nsns.lineplot(data=rm_x_date,x='Month_Year', y='User Subscriptions Removed', label ='Subs Removed')\nsns.lineplot(data=likes_rm_date,x='Month_Year', y='Video Likes Removed', label = 'Likes Removed')\nsns.lineplot(data=dislikes_date,x='Month_Year', y='Video Dislikes Added', label = 'Dislikes')\n\n#we can see that there was still a spike in the normalized metrics during the same point in the year. It is worth exploring this further! ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:21:46.496591Z","iopub.execute_input":"2022-08-31T20:21:46.496911Z","iopub.status.idle":"2022-08-31T20:21:48.269466Z","shell.execute_reply.started":"2022-08-31T20:21:46.496873Z","shell.execute_reply":"2022-08-31T20:21:48.268471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\nIn this segment you learned the following:\n- How to analyze individual features using histograms, boxplots, and bar charts\n- How to see relationships between features using scatterplots, correlation heatmaps, bar charts, and line plots \n- How to use these charts to seek additional insight from your data and explore what relationships to dive deeper into ","metadata":{}},{"cell_type":"markdown","source":"## Comment Data","metadata":{}},{"cell_type":"markdown","source":"## Your Turn! \n### I started you off with the basics of Exploratory Data Analsys (EDA), it's your turn to add your insights to the analysis. Excited to see what you come up with!!\nI'm constantly thinking about questions like:\n- What topics get the most viewership?\n- What do are people asking for in the comments?\n- What impacts watch time and click through rate?\n- Can we predict if a title will be clickable? \n- What thumbnails are most appealing (thumbnail data available in df_agg)\n- What is different about my \"viral\" videos and normal videos\n- Who is the core audience of my channel?\n","metadata":{}}]}