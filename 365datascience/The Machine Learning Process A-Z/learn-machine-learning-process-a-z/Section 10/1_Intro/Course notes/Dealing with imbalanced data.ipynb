{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-10T00:24:00.277431Z","iopub.execute_input":"2022-11-10T00:24:00.277795Z","iopub.status.idle":"2022-11-10T00:24:00.290369Z","shell.execute_reply.started":"2022-11-10T00:24:00.277768Z","shell.execute_reply":"2022-11-10T00:24:00.289637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with Imbalanced Data\nIn this notebook, we will go through some of the basic techinques for dealing with imbalanced datasets. This is a companion workbook for the 365 Data Science course on ML Process. This notebook only foucses on implementation. Check out the course or the documentation for the in-depth explanations of each approach. \n\nWe will cover:\n- Random Oversampling\n- Random Undersampling \n- Synthetic Minority Oversampling (SMOTE)\n- Borderline SMOTE\n- Adaptive Synthetic Oversampling \n\nimblearn Documentation: https://imbalanced-learn.org/stable/\n\n### On the Data \nThis dataset is a good representation of what you may see in the real world. Most data will have some imbalances, and in certain domains you will see a lot of inequality. You will likely see large imbalances in cases of expensive purchases or fraud detection.","metadata":{}},{"cell_type":"code","source":"#Read in Data \ndf_train = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:24:40.498260Z","iopub.execute_input":"2022-11-10T00:24:40.498675Z","iopub.status.idle":"2022-11-10T00:24:40.533358Z","shell.execute_reply.started":"2022-11-10T00:24:40.498643Z","shell.execute_reply":"2022-11-10T00:24:40.531818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see below, the data is imbalanced with far more 0's than 1's. While there isn't a massive imbalance, using different sampling methods could help improve our model results. ","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:24:42.833079Z","iopub.execute_input":"2022-11-10T00:24:42.833778Z","iopub.status.idle":"2022-11-10T00:24:42.853956Z","shell.execute_reply.started":"2022-11-10T00:24:42.833742Z","shell.execute_reply":"2022-11-10T00:24:42.852798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:25:05.127848Z","iopub.execute_input":"2022-11-10T00:25:05.129227Z","iopub.status.idle":"2022-11-10T00:25:05.139740Z","shell.execute_reply.started":"2022-11-10T00:25:05.129168Z","shell.execute_reply":"2022-11-10T00:25:05.138695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create validation dataset to see how different sampling methocds could impact model outcomes\nfrom sklearn.model_selection import train_test_split\n\nX = pd.get_dummies(df_train.drop(['enrollee_id','target'], axis =1))\ny = df_train[['target']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:25:28.783783Z","iopub.execute_input":"2022-11-10T00:25:28.784179Z","iopub.status.idle":"2022-11-10T00:25:28.868091Z","shell.execute_reply.started":"2022-11-10T00:25:28.784150Z","shell.execute_reply":"2022-11-10T00:25:28.866768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see imblance in data majority is not looking for a job change (0)\ny_train.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:25:39.718048Z","iopub.execute_input":"2022-11-10T00:25:39.718434Z","iopub.status.idle":"2022-11-10T00:25:39.855259Z","shell.execute_reply.started":"2022-11-10T00:25:39.718406Z","shell.execute_reply":"2022-11-10T00:25:39.853642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:25:58.086915Z","iopub.execute_input":"2022-11-10T00:25:58.087345Z","iopub.status.idle":"2022-11-10T00:25:58.094044Z","shell.execute_reply.started":"2022-11-10T00:25:58.087313Z","shell.execute_reply":"2022-11-10T00:25:58.093085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model to validate on \n","metadata":{"execution":{"iopub.status.busy":"2022-10-10T03:59:49.313474Z","iopub.execute_input":"2022-10-10T03:59:49.313888Z","iopub.status.idle":"2022-10-10T03:59:49.318039Z","shell.execute_reply.started":"2022-10-10T03:59:49.313834Z","shell.execute_reply":"2022-10-10T03:59:49.316976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameters \n\n### For all sampling models\n\nsampling_strategy: float, str, dict or callable, default=’auto’\n\n>- float\n    - When float, it corresponds to the desired ratio of the number of samples in the minority class over \n    the number of samples in the majority class after resampling. Therefore, the ratio is expressed as \n    \\alpha_{os} = N_{rm} / N_{M} where N_{rm} is the number of samples in the minority class after \n    resampling and N_{M} is the number of samples in the majority class.\n>- str\n>    - 'minority': resample only the minority class;\n>\n>    - 'not minority': resample all classes but the minority class;\n>\n>    - 'not majority': resample all classes but the majority class;\n>\n>    - 'all': resample all classes;\n>\n>    - 'auto': equivalent to 'not majority'. \n>- dict\n    - When dict, the keys correspond to the targeted classes. The values correspond to the desired number\n    of samples for each targeted class.\n\nrandom_state: int, RandomState instance, default=None \n- int --> Most cases use this to set a consistent random state\n\nshrinkage: float or dict, default=None\n- Shrinkage allows us to add slight noise to our data so that our resampled variables don't perfectly overlap. This could be useful for normalization of our results. A good example of how this works is located here: https://imbalanced-learn.org/stable/auto_examples/over-sampling/plot_shrinkage_effect.html\n>- if **None**, a normal bootstrap will be generated without perturbation. It is equivalent to shrinkage=0 as well\n>- if a **float** is given, the shrinkage factor will be used for all classes to generate the smoothed bootstrap;\n>- if a **dict** is given, the shrinkage factor will specific for each class. The key correspond to the targeted class and the value is the shrinkage factor.\n\n### For Smote and Borderline Smote\nk_neighbor: sint or object, default=5\n>- If int, number of nearest neighbours to used to construct synthetic samples. If object, an estimator that inherits from KNeighborsMixin that will be used to find the k_neighbors.\n\nn_jobs: int, default=None\n>- Number of CPU cores used during the cross-validation loop. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n\n### For Borderline Smote\nkind: {“borderline-1”, “borderline-2”}, default=’borderline-1’\n>- The type of SMOTE algorithm to use one of the following options: 'borderline-1', 'borderline-2'.\n>    -Breakdown of difference between borderline-1 and borderline-2: https://sci2s.ugr.es/keel/pdf/specific/congreso/han_borderline_smote.pdf\n\nCopied from Docs: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\nSmote: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\nBorderline Smote: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html","metadata":{}},{"cell_type":"markdown","source":"# Random Oversampling\nRandom Oversampling is simply randomly picking data points in our minority class and duplicating them. ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\no_smpl = RandomOverSampler(random_state = 42) \n\nX_o_smpl, y_o_smpl = o_smpl.fit_resample(X_train,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:12.012605Z","iopub.execute_input":"2022-11-10T00:34:12.013014Z","iopub.status.idle":"2022-11-10T00:34:12.102417Z","shell.execute_reply.started":"2022-11-10T00:34:12.012969Z","shell.execute_reply":"2022-11-10T00:34:12.101453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_o_smpl.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:29:58.264413Z","iopub.execute_input":"2022-11-10T00:29:58.264831Z","iopub.status.idle":"2022-11-10T00:29:58.399156Z","shell.execute_reply.started":"2022-11-10T00:29:58.264800Z","shell.execute_reply":"2022-11-10T00:29:58.398389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Undersampling\nRandom Undersampling is simply randomly picking data points in our majority class and removing them. ","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nu_smpl = RandomUnderSampler(random_state = 42) \n\nX_u_smpl, y_u_smpl = u_smpl.fit_resample(X_train,y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:19.981172Z","iopub.execute_input":"2022-11-10T00:34:19.981582Z","iopub.status.idle":"2022-11-10T00:34:20.044780Z","shell.execute_reply.started":"2022-11-10T00:34:19.981549Z","shell.execute_reply":"2022-11-10T00:34:20.043285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#note how many less total rows we have now \ny_u_smpl.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:30:25.528975Z","iopub.execute_input":"2022-11-10T00:30:25.529404Z","iopub.status.idle":"2022-11-10T00:30:25.670538Z","shell.execute_reply.started":"2022-11-10T00:30:25.529362Z","shell.execute_reply":"2022-11-10T00:30:25.669428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Synthetic Minority Oversampling (SMOTE)\nSynthetic Minority Oversampling (SMOTE) is an oversampling technique that creates synthetic data points. SMOTE address’ the core problem in oversampling. Oversampling creates duplicate datapoints whereas SMOTE slightly alters these data points. ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE \n\nsmote = SMOTE(random_state = 42) \n\nX_smote, y_smote = smote.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:23.383704Z","iopub.execute_input":"2022-11-10T00:34:23.384083Z","iopub.status.idle":"2022-11-10T00:34:23.780124Z","shell.execute_reply.started":"2022-11-10T00:34:23.384055Z","shell.execute_reply":"2022-11-10T00:34:23.779071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_smote.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:31:08.691580Z","iopub.execute_input":"2022-11-10T00:31:08.692027Z","iopub.status.idle":"2022-11-10T00:31:08.839747Z","shell.execute_reply.started":"2022-11-10T00:31:08.691996Z","shell.execute_reply":"2022-11-10T00:31:08.838886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Borderline Smote\nThe idea behind borderline SMOTE is that we only want to use data that’s at risk of being misclassified as the data to be oversampled. In this case, we build a classifier to classify points as positive or negative. Then, for the data points we misclassify, we oversample those data points. This would hopefully train our algorithm to better recognize these difficult instances and correct for them. ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import BorderlineSMOTE\n\nbsmote = BorderlineSMOTE(random_state = 42) \n\nX_bsmote, y_bsmote = bsmote.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:27.566960Z","iopub.execute_input":"2022-11-10T00:34:27.567601Z","iopub.status.idle":"2022-11-10T00:34:29.062360Z","shell.execute_reply.started":"2022-11-10T00:34:27.567559Z","shell.execute_reply":"2022-11-10T00:34:29.060548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_bsmote.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:31:53.105813Z","iopub.execute_input":"2022-11-10T00:31:53.106704Z","iopub.status.idle":"2022-11-10T00:31:53.243382Z","shell.execute_reply.started":"2022-11-10T00:31:53.106661Z","shell.execute_reply":"2022-11-10T00:31:53.242235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adaptive Synthetic Oversampling (ADASYN)\nThe idea behind AdaSyn is to use a weight distribution of our minority class. Essentially, we give higher weight to instances that are more difficult to learn and lower weight to instances that are easier to learn. AdaSyn is very similar to safe-level SMOTE, except there’s just a different way of computing the synthetic data points. ","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import ADASYN \nadasyn = ADASYN(random_state = 42) \n\nX_ada, y_ada = adasyn.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:31.901534Z","iopub.execute_input":"2022-11-10T00:34:31.903098Z","iopub.status.idle":"2022-11-10T00:34:33.466763Z","shell.execute_reply.started":"2022-11-10T00:34:31.903033Z","shell.execute_reply":"2022-11-10T00:34:33.465628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_ada.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:32:47.919815Z","iopub.execute_input":"2022-11-10T00:32:47.920209Z","iopub.status.idle":"2022-11-10T00:32:48.053857Z","shell.execute_reply.started":"2022-11-10T00:32:47.920176Z","shell.execute_reply":"2022-11-10T00:32:48.052845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How do these compare?\nLet's quickly compare our sampling techniques by looking at the differences in the sampling data. Then let's compare the results of after feeding them into a model. This is just a little experiment, but it can show how much of a difference sampling can make depending on your data and models. \n\nLet's train a simple random forest on each of the different types of samples, then let's see the difference in validation accuracy. ","metadata":{}},{"cell_type":"code","source":"#let's explore a few variables across these 4 different oversampling techniques across two variables 'city_development_index' and 'training_hours'\n\nprint('city_development_index Differences:')\nfor i in [X_o_smpl, X_smote, X_bsmote, X_ada]:\n    print(i.mean()['city_development_index'])\n\nprint('training_hours Differences:')\nfor i in [X_o_smpl, X_smote, X_bsmote, X_ada]:\n    print(i.mean()['training_hours'])\n    \n#what is important here is that there are differences in the sampling of each techinque, not where the differences come in these two variables ","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:33:03.971072Z","iopub.execute_input":"2022-11-10T00:33:03.971446Z","iopub.status.idle":"2022-11-10T00:33:04.100020Z","shell.execute_reply.started":"2022-11-10T00:33:03.971415Z","shell.execute_reply":"2022-11-10T00:33:04.099039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n#oversampling model \nclf_os = RandomForestClassifier(random_state=0)\nclf_os.fit(X_o_smpl, y_o_smpl)\nos_pred = clf_os.predict(X_test)\n\n#undersampling model \nclf_us = RandomForestClassifier(random_state=0)\nclf_us.fit(X_u_smpl, y_u_smpl)\nus_pred = clf_us.predict(X_test)\n\n#SMOTE \nclf_sm = RandomForestClassifier(random_state=0)\nclf_sm.fit(X_smote, y_smote)\nsm_pred = clf_sm.predict(X_test)\n\n#borderline SMOTE \nclf_bsm = RandomForestClassifier(random_state=0)\nclf_bsm.fit(X_bsmote, y_bsmote)\nbsm_pred = clf_bsm.predict(X_test)\n\n#ADDASYN\nclf_ada = RandomForestClassifier(random_state=0)\nclf_ada.fit(X_ada, y_ada)\nada_pred = clf_ada.predict(X_test)\n\nprint('Oversampling Score: %.3f' % accuracy_score(y_test,os_pred))\nprint('Undersampling Score: %.3f' % accuracy_score(y_test,us_pred))\nprint('SMOTE Score: %.3f '% accuracy_score(y_test,sm_pred))\nprint('Borderline SMOTE Score: %.3f'% accuracy_score(y_test,bsm_pred))\nprint('ADASYN Score: %.3f'% accuracy_score(y_test,ada_pred))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:34:53.039680Z","iopub.execute_input":"2022-11-10T00:34:53.040093Z","iopub.status.idle":"2022-11-10T00:35:13.865488Z","shell.execute_reply.started":"2022-11-10T00:34:53.040063Z","shell.execute_reply":"2022-11-10T00:35:13.863767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(random_state=0)\nclf.fit(X_train, y_train)\npred = clf_ada.predict(X_test)\nprint('No Change Score: %.3f'% accuracy_score(y_test,pred))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:36:16.081312Z","iopub.execute_input":"2022-11-10T00:36:16.081675Z","iopub.status.idle":"2022-11-10T00:36:19.351514Z","shell.execute_reply.started":"2022-11-10T00:36:16.081647Z","shell.execute_reply":"2022-11-10T00:36:19.350304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now let's tune some features in SMOTE to see if we get slightly better results. Let's do this with SMOTE","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:36:46.602806Z","iopub.execute_input":"2022-11-10T00:36:46.604115Z","iopub.status.idle":"2022-11-10T00:36:46.608545Z","shell.execute_reply.started":"2022-11-10T00:36:46.604071Z","shell.execute_reply":"2022-11-10T00:36:46.607466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,10,1):\n    smote = SMOTE(k_neighbors = i, random_state=42) \n\n    X_smote, y_smote = smote.fit_resample(X_train,y_train)\n\n    clf_sm = RandomForestClassifier(random_state=0)\n    clf_sm.fit(X_smote, y_smote.values.ravel())\n    sm_pred = clf_sm.predict(X_test)\n    print('K = ' +str(i))\n    print('SMOTE Score: %.3f '% accuracy_score(y_test,sm_pred))\n    \n#it seems like k = 5 produced the best results. ","metadata":{"execution":{"iopub.status.busy":"2022-11-10T00:37:04.867808Z","iopub.execute_input":"2022-11-10T00:37:04.868182Z","iopub.status.idle":"2022-11-10T00:37:54.419029Z","shell.execute_reply.started":"2022-11-10T00:37:04.868155Z","shell.execute_reply":"2022-11-10T00:37:54.417578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}