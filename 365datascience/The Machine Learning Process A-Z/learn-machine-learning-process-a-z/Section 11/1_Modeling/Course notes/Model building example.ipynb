{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007bb7a5",
   "metadata": {
    "papermill": {
     "duration": 0.008212,
     "end_time": "2022-11-19T00:32:33.806260",
     "exception": false,
     "start_time": "2022-11-19T00:32:33.798048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The ML Modeling Process Basics \n",
    "In this notebook, we will go through some of the basic techinques for modeling data. This is a companion workbook for the 365 Data Science course on ML Process. This notebook only foucses on implementation. Check out the course or the documentation for the in-depth explanations of each step\n",
    "\n",
    "In this case, we will be trying to predict if we can predict a stroke from the above dataset. \n",
    "\n",
    "We will cover:\n",
    "- Baseline creation\n",
    "- Model selection\n",
    "- Parameter tuning\n",
    "     - manual\n",
    "     - gridsearch\n",
    "     - random search\n",
    "     - basian optomization\n",
    "- Ensemble models\n",
    "\n",
    "### On the Data \n",
    "This dataset is a good representation of real world data that can have valuable impact when analyzed. We will be exploring the accuracy of different models for predicting if someone will purchase an auto insurance policy or not. We will first lightly explore the data, create our train, test / validation sets, then we will ceate a baseline model. To get the best results we will compare other algorithms to our basline and use various parameter tuning techniques to see which model produces the best results. At the end we will explore some ensemble models to see what produces the best results. \n",
    "\n",
    "The focus of this notebook is the modeling process. If you're interested in the specifics of differen machine learning algorithms, check out our other course specifically on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde14acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36cfc47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T00:32:33.826578Z",
     "iopub.status.busy": "2022-11-19T00:32:33.826069Z",
     "iopub.status.idle": "2022-11-19T00:32:33.883660Z",
     "shell.execute_reply": "2022-11-19T00:32:33.882125Z"
    },
    "papermill": {
     "duration": 0.07048,
     "end_time": "2022-11-19T00:32:33.885364",
     "exception": true,
     "start_time": "2022-11-19T00:32:33.814884",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cross_Sell_Prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9edd97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:24.571575Z",
     "iopub.status.busy": "2022-11-14T21:04:24.570837Z",
     "iopub.status.idle": "2022-11-14T21:04:24.770187Z",
     "shell.execute_reply": "2022-11-14T21:04:24.769066Z",
     "shell.execute_reply.started": "2022-11-14T21:04:24.571527Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#look at basic data for continuous variables \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a20fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:24.774188Z",
     "iopub.status.busy": "2022-11-14T21:04:24.773681Z",
     "iopub.status.idle": "2022-11-14T21:04:24.972770Z",
     "shell.execute_reply": "2022-11-14T21:04:24.971621Z",
     "shell.execute_reply.started": "2022-11-14T21:04:24.774142Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2442d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:24.975629Z",
     "iopub.status.busy": "2022-11-14T21:04:24.974771Z",
     "iopub.status.idle": "2022-11-14T21:04:25.087532Z",
     "shell.execute_reply": "2022-11-14T21:04:25.086259Z",
     "shell.execute_reply.started": "2022-11-14T21:04:24.975583Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#small enough number of null values we will just remove them.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8bf6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.090181Z",
     "iopub.status.busy": "2022-11-14T21:04:25.089378Z",
     "iopub.status.idle": "2022-11-14T21:04:25.183340Z",
     "shell.execute_reply": "2022-11-14T21:04:25.182108Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.090136Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for possible nulls in categoricals / non answers \n",
    "for i in df.select_dtypes(include=['object']).columns:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d35b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.185453Z",
     "iopub.status.busy": "2022-11-14T21:04:25.184964Z",
     "iopub.status.idle": "2022-11-14T21:04:25.205941Z",
     "shell.execute_reply": "2022-11-14T21:04:25.204355Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.185411Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dropping variable for now, but could likely be used to imrove our models with some engineering! \n",
    "df.Policy_Sales_Channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427bdaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.208101Z",
     "iopub.status.busy": "2022-11-14T21:04:25.207619Z",
     "iopub.status.idle": "2022-11-14T21:04:25.231465Z",
     "shell.execute_reply": "2022-11-14T21:04:25.230230Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.208051Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_trimmed = df.loc[:,['Gender','Age','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Vintage','Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe22ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.234014Z",
     "iopub.status.busy": "2022-11-14T21:04:25.232908Z",
     "iopub.status.idle": "2022-11-14T21:04:25.466668Z",
     "shell.execute_reply": "2022-11-14T21:04:25.465545Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.233957Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop null values and create dummy variables\n",
    "df_final = pd.get_dummies(df_trimmed).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852886b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.469955Z",
     "iopub.status.busy": "2022-11-14T21:04:25.469555Z",
     "iopub.status.idle": "2022-11-14T21:04:25.478026Z",
     "shell.execute_reply": "2022-11-14T21:04:25.476682Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.469920Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a93979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.480171Z",
     "iopub.status.busy": "2022-11-14T21:04:25.479762Z",
     "iopub.status.idle": "2022-11-14T21:04:25.496952Z",
     "shell.execute_reply": "2022-11-14T21:04:25.495952Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.480133Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.Response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81da22b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:04:25.499226Z",
     "iopub.status.busy": "2022-11-14T21:04:25.498368Z",
     "iopub.status.idle": "2022-11-14T21:04:26.877315Z",
     "shell.execute_reply": "2022-11-14T21:04:26.876135Z",
     "shell.execute_reply.started": "2022-11-14T21:04:25.499190Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create train test split \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_final.drop('Response', axis =1)\n",
    "y = df_final.loc[:,['Response']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c039549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T06:18:52.081557Z",
     "iopub.status.busy": "2022-11-12T06:18:52.081225Z",
     "iopub.status.idle": "2022-11-12T06:18:52.087894Z",
     "shell.execute_reply": "2022-11-12T06:18:52.087053Z",
     "shell.execute_reply.started": "2022-11-12T06:18:52.081524Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#balance the data (SMOTE) Try this if interested: https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10\n",
    "\"\"\"from imblearn.over_sampling import SMOTE \n",
    "smote = SMOTE(sampling_strategy =1)\n",
    "\n",
    "X_train, y_train = smote.fit_resample(X_train,y_train)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bfd5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Creating a Basline Model\n",
    "How can we tell if our machine learning models are any good? To evaluate performance, we need to benchmark against something. In this case, we will create two baslines for our model. First, we can simply look at the average of our data for a numeric value. If we were going to predict the age, we could simply guess the average age for every candidate. \n",
    "\n",
    "On the other hand, for a categorical variable, we could simply guess 50/50 or the ratio of the categories in the data. In this case, the conversion data is imbalanced with 46710/ 334399 samples being of the non-stroke cateogry. That means that if we guessed that everyone in the sample didn't have a stroke, we would have a 86.0% success rate. Since this data is slightly imblanaced, this would not be a good baseline for our model.\n",
    "\n",
    "One of the most important steps that we need to take is choosing a good evluation metric. The notebook that covers specific evaluation metrics can be located here: \n",
    "\n",
    "Accuracy does not make sense because of the imbalanced nature of the data. For this example we will use F1 score as our model evaluation metric.\n",
    "\n",
    "- F1 is calculated by 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "- Instead of a simple accuracy calculation which would give us a baseline of 96.1%, F1 score gives us an undefined number since both the precision and recall of a model that only predicted negatives would equal 0. \n",
    "\n",
    "- In this case, we want to use a simple basleline model like Naive Bayes to set our baseline based off of f1 score. You can use most models to create a baseline, but I like Naive bayes because it is quick and doesn't require much parameter tuning. (Full breakdown of Naieve Bayes in or Algorithms Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c649d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:05:17.104645Z",
     "iopub.status.busy": "2022-11-14T21:05:17.104128Z",
     "iopub.status.idle": "2022-11-14T21:05:18.183001Z",
     "shell.execute_reply": "2022-11-14T21:05:18.181849Z",
     "shell.execute_reply.started": "2022-11-14T21:05:17.104601Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#import Naive Bayes Classifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#create classifier object\n",
    "nb = GaussianNB()\n",
    "\n",
    "#run cv for NB classifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_accuracy = cross_val_score(nb,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "nb_f1 = cross_val_score(nb,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('nb_accuracy: ' +str(nb_accuracy))\n",
    "print('nb F1_Macro Score: '+str(nb_f1))\n",
    "print('nb_accuracy_avg: ' + str(nb_accuracy.mean()) +'  |  lr_f1_avg: '+str(nb_f1.mean()))\n",
    "\n",
    "\n",
    "#With these F1 scores, we can begin evaluating our model. While the accuracy is lower than if we only predicted 0 every time,\n",
    "# our f1 score suggests we are doing a far better job of predicting stroke outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6958b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model Comparison & Selection \n",
    "After we have a baseline model to compare against, we want to evaluate how other models might perform on the same data. I like to experiment with other basic models with very little paramater tuning to see what performs well. This isn't an exact science and many people may do this step differently. After we set up the models, we can begin experimenting with parameter tuning. I find that model selection and parameter tuning is often an iterative process. For an analysis like this, trying different models, changing parameters, and experimenting with new engineered features is where I find myself spending most of my time working. \n",
    "\n",
    "In this section we will try:\n",
    "- Logistic regression\n",
    "- Decision Tree\n",
    "- K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807b4a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:05:22.671411Z",
     "iopub.status.busy": "2022-11-14T21:05:22.670890Z",
     "iopub.status.idle": "2022-11-14T21:09:58.303629Z",
     "shell.execute_reply": "2022-11-14T21:09:58.302354Z",
     "shell.execute_reply.started": "2022-11-14T21:05:22.671372Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's now experiment with a few different basic models \n",
    "\n",
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state =32)\n",
    "\n",
    "dt_accuracy = cross_val_score(dt,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "dt_f1 = cross_val_score(dt,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('dt_accuracy: ' +str(dt_accuracy))\n",
    "print('dt F1_Macro Score: '+str(dt_f1))\n",
    "print('dt_accuracy_avg: ' + str(dt_accuracy.mean()) +'  |  dt_f1_avg: '+str(dt_f1.mean())+'\\n')\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=32, max_iter = 2000, class_weight = 'balanced')\n",
    "\n",
    "lr_accuracy = cross_val_score(lr,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "lr_f1 = cross_val_score(lr,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('lr_accuracy: ' +str(lr_accuracy))\n",
    "print('lr F1_Macro Score: '+str(lr_f1))\n",
    "print('lr_accuracy_avg: ' + str(lr_accuracy.mean()) +'  |  lr_f1_avg: '+str(lr_f1.mean())+'\\n')\n",
    "\n",
    "\n",
    "## KNN \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "knn_accuracy = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "knn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('knn_accuracy: ' +str(knn_accuracy))\n",
    "print('knn F1_Macro Score: '+str(knn_f1))\n",
    "print('knn_accuracy_avg: ' + str(knn_accuracy.mean()) +'  |  knn_f1_avg: '+str(knn_f1.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974370c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model Comparison \n",
    "It looks like we chose a pretty good baseline. While it slightly underperforms all of our new models in accuracy, it outperforms all of them in F1 score which is what we care about most for this analysis. Let's look at how everything stacks up. \n",
    "\n",
    "|Model          | F1 Score      |\n",
    "| :------------ | :-----------: |\n",
    "| **Baseline Naive Bayes**  | **41.3%**     |\n",
    "| Logistic Regression  | **35.0%**     |\n",
    "| Decision Tree  | **27.6%**     |\n",
    "| K Nearest Neighbors | **24.6%**     |\n",
    "\n",
    "\n",
    "\n",
    "While all of our models outperformed our basline, we still can do better. We can now parameter tune! That means that we make adjustments to the model parameter inputs to better compensate for our specific data. One of the drawbacks of Naive Bayes is that it has virtually no paramaters that we can tune, so our inital results are about the best we will get with it without making changes to our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b76da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Manual Parameter Tuning\n",
    "Let's try to do some parameter tuning with a few of these models:\n",
    "\n",
    "Let's start with K Nearest Neighbors,which has a few parameters we can adjust, one of them being the number of K. K is how many other datapoints it uses to make its classification. If k= 3 it uses it sees what the samples 3 closest neighbors is and classifies it as the most common one. If k = 5, it uses its 5 closest datapoints. Let's change the number of k and see if that changes our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce224bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:10:16.048927Z",
     "iopub.status.busy": "2022-11-14T21:10:16.048428Z",
     "iopub.status.idle": "2022-11-14T21:20:32.940488Z",
     "shell.execute_reply": "2022-11-14T21:20:32.937734Z",
     "shell.execute_reply.started": "2022-11-14T21:10:16.048880Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Knn Model Comparison \n",
    "\n",
    "#here we will loop through and see which value of k performs the best. \n",
    "\n",
    "for i in range(1,7):\n",
    "    knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=i))\n",
    "    knn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "    print('K ='+(str(i)) + (': ') + str(knn_f1.mean()))\n",
    "\n",
    "#What we find is that k=1 is the best estimator for this specific model. We go from 24.6% to 27.6%, a decent improvement! \n",
    "#We also realize that KNN may not be the best approach here because of the imbalanced data. \n",
    "#The larger the K is, the more of the majority class will automatically be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ca0ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Randomized Parameter Tuning\n",
    "Since KNN may not be the best choice, let's explore the deicision tree. Decision trees have a lot more features we can tune. We can tweak the following:\n",
    "- criterion {gini, entropy, log loss}\n",
    "- splitter {best, random}\n",
    "- max depth {int, None}\n",
    "- min_samples_split {int, None}\n",
    "- min_samples_leave {int, None}\n",
    "- min_weight_fraction_leaf {float}\n",
    "- max_features {int, auto, sqrt, log2, None}\n",
    "- max_leaf_nodes {int, None}\n",
    "- min_impurity_decrease {float}\n",
    "- class_weight {dict, balanced, None}\n",
    "- ccp_alpha {float}\n",
    "\n",
    "There are a lot of parameters to tune! If there are just 2 options for each one that would be 2^11, which is 2048 total configurations. In theory, there are infinate numbers of paramater configurations. How do we even get close to finding the best one? \n",
    "\n",
    "The answer here is randomized search. We through in all the parameters that we are interested in searching, and the model will randomly select a subset and return the one that produces the best results. \n",
    "\n",
    "Still, let's manually select a few paramaters we want to evaluate on and then use randomized search:\n",
    "- criterion\n",
    "- split strategy\n",
    "- max depth\n",
    "- min_samples_split\n",
    "- max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e655659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:22:40.317593Z",
     "iopub.status.busy": "2022-11-14T21:22:40.316056Z",
     "iopub.status.idle": "2022-11-14T21:24:42.439199Z",
     "shell.execute_reply": "2022-11-14T21:24:42.437722Z",
     "shell.execute_reply.started": "2022-11-14T21:22:40.317536Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "features = {'criterion': ['gini','entropy'],\n",
    "            'splitter': ['best','random'],\n",
    "           'max_depth': [2,5,10,20,40,None],\n",
    "           'min_samples_split': [2,5,10,15],\n",
    "           'max_features': ['auto','sqrt','log2',None]}\n",
    "\n",
    "rs_dt = RandomizedSearchCV(estimator = dt, param_distributions =features, n_iter =100, cv = 3, random_state = 42, scoring ='f1')\n",
    "\n",
    "rs_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a548138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:42.442158Z",
     "iopub.status.busy": "2022-11-14T21:24:42.441806Z",
     "iopub.status.idle": "2022-11-14T21:24:42.449692Z",
     "shell.execute_reply": "2022-11-14T21:24:42.448334Z",
     "shell.execute_reply.started": "2022-11-14T21:24:42.442127Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('best stcore = ' + str(rs_dt.best_score_))\n",
    "print('best params = ' + str(rs_dt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e06f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# GridsearchCV (Exhaustive Parameter Tuning)\n",
    "With this we have improved our model f1 score from **27.6% to 27.9%**. This is a decent increase! We also narrowed down some of the features that produced good results. We may want to try a more exhaustive search this time. Gridsearch goes through all of the possible combinations within an range and returns the best outcome. \n",
    "\n",
    "This time, let's do an exhaustive search of a smaller number of features and see if we can improve our results even more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834d6c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:28:43.905689Z",
     "iopub.status.busy": "2022-11-14T21:28:43.905188Z",
     "iopub.status.idle": "2022-11-14T21:32:26.827756Z",
     "shell.execute_reply": "2022-11-14T21:32:26.826652Z",
     "shell.execute_reply.started": "2022-11-14T21:28:43.905650Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "features_gs = {'criterion': ['entropy'],\n",
    "            'splitter': ['random'],\n",
    "           'max_depth': np.arange(30,50,1), #getting more precise within range\n",
    "           'min_samples_split': [2,3,4,5,6,7,8,9],\n",
    "           'max_features': [None]}\n",
    "\n",
    "gs_dt = GridSearchCV(estimator = dt, param_grid =features_gs, cv = 3, scoring ='f1') #we don't need random state because there isn't randomization like before\n",
    "\n",
    "gs_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c567d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:32:26.830018Z",
     "iopub.status.busy": "2022-11-14T21:32:26.829612Z",
     "iopub.status.idle": "2022-11-14T21:32:26.835388Z",
     "shell.execute_reply": "2022-11-14T21:32:26.834200Z",
     "shell.execute_reply.started": "2022-11-14T21:32:26.829987Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('best stcore = ' + str(gs_dt.best_score_))\n",
    "print('best params = ' + str(gs_dt.best_params_))\n",
    "\n",
    "#looks like we can  do a little better with this gridsearch! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac497c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Bayesian Optimization\n",
    "I wonnder if we can do better than the funnel approach that we took with random search and gridsearch. What if we used a slightly smarter algorithm to help evaluate our features. Maybe we could explore all of the variables from the previous examples and see if our model missed something. This is where Bayesian Optimization comes in. This is an iterative process where our model improves its understandings of the feature inputs as it goes. (Full breakdown in the video portion of the course)\n",
    "\n",
    "Now let's try to use this with a larger feature set on the same classifier. This won't guarantee a better result as it still is not an exahustive search, but in theory it let's us cover ground in a more efficient way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb61345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:33:16.995367Z",
     "iopub.status.busy": "2022-11-14T21:33:16.993915Z",
     "iopub.status.idle": "2022-11-14T21:33:17.147013Z",
     "shell.execute_reply": "2022-11-14T21:33:17.145784Z",
     "shell.execute_reply.started": "2022-11-14T21:33:16.995315Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300ba5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:38:46.396174Z",
     "iopub.status.busy": "2022-11-14T21:38:46.395707Z",
     "iopub.status.idle": "2022-11-14T21:47:41.022625Z",
     "shell.execute_reply": "2022-11-14T21:47:41.021260Z",
     "shell.execute_reply.started": "2022-11-14T21:38:46.396140Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Choose cross validation method \n",
    "cv = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "\n",
    "bs_lr = BayesSearchCV(\n",
    "    dt,\n",
    "    {'criterion': Categorical(['gini','entropy']),\n",
    "            'splitter': Categorical(['best','random']),\n",
    "           'max_depth': Integer(10,50),\n",
    "           'min_samples_split': Integer(2,15),\n",
    "           'max_features': Categorical(['auto','sqrt','log2',None])},\n",
    "    random_state=42,\n",
    "    n_iter= 100,\n",
    "    cv= cv,\n",
    "    scoring ='f1')\n",
    " \n",
    "bs_lr.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f90edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:47:53.217446Z",
     "iopub.status.busy": "2022-11-14T21:47:53.216927Z",
     "iopub.status.idle": "2022-11-14T21:47:53.224828Z",
     "shell.execute_reply": "2022-11-14T21:47:53.223240Z",
     "shell.execute_reply.started": "2022-11-14T21:47:53.217407Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('best stcore = ' + str(bs_lr.best_score_))\n",
    "print('best params = ' + str(bs_lr.best_params_))\n",
    "\n",
    "#while this didn't outperform our gridsearch, it is still a good approach to try when dealing with many different feature options. \n",
    "#it still did outperform our originial random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589eb05d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Selecting a Model\n",
    "We still haven't been able to do better than our baseline. In most cases, we to tune multiple different models until we reach one that performs the best based on our evaluation criteria. We also want to use other considerations like training time, prediction time, prediction time or interperetability to select selct the best model for our use case. \n",
    "\n",
    "Since we have one tuned model, lets see if we can improve it by combining it with a few of the other models we have used. This process is called ensembling. In the case of classification, we often use a popular vote metric to select the best model. \n",
    "\n",
    "Let's see if an ensemble model of these three classifiers outperforms our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a61a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:49:51.931371Z",
     "iopub.status.busy": "2022-11-14T21:49:51.930912Z",
     "iopub.status.idle": "2022-11-14T21:49:51.939880Z",
     "shell.execute_reply": "2022-11-14T21:49:51.938327Z",
     "shell.execute_reply.started": "2022-11-14T21:49:51.931333Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dt_voting = DecisionTreeClassifier(**{'criterion': 'entropy', 'max_depth': 44, 'max_features': None, 'min_samples_split': 2, 'splitter': 'random'}) # ** allows you to pass in parameters as dict\n",
    "knn_voting = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
    "lr_voting = LogisticRegression(random_state=32, max_iter = 2000, class_weight = 'balanced')\n",
    "\n",
    "ens = VotingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('lr',lr_voting)], voting = 'hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0174db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:49:53.246105Z",
     "iopub.status.busy": "2022-11-14T21:49:53.245150Z",
     "iopub.status.idle": "2022-11-14T21:54:28.643376Z",
     "shell.execute_reply": "2022-11-14T21:54:28.642203Z",
     "shell.execute_reply.started": "2022-11-14T21:49:53.246062Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "voting_accuracy = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "voting_f1 = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('voting_accuracy: ' +str(voting_accuracy))\n",
    "print('voting F1_Macro Score: '+str(voting_f1))\n",
    "print('voting_accuracy_avg: ' + str(voting_accuracy.mean()) +'  |  voting_f1_avg: '+str(voting_f1.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646dbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:55:30.738930Z",
     "iopub.status.busy": "2022-11-14T21:55:30.738516Z",
     "iopub.status.idle": "2022-11-14T21:58:59.710747Z",
     "shell.execute_reply": "2022-11-14T21:58:59.709131Z",
     "shell.execute_reply.started": "2022-11-14T21:55:30.738899Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ens = VotingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('lr',lr_voting)], voting = 'soft')\n",
    "voting_accuracy = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "voting_f1 = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('voting_accuracy: ' +str(voting_accuracy))\n",
    "print('voting F1_Macro Score: '+str(voting_f1))\n",
    "print('voting_accuracy_avg: ' + str(voting_accuracy.mean()) +'  |  voting_f1_avg: '+str(voting_f1.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620a3f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Stacked classifier \n",
    "In the case of the voting classifer, we didn't get better performance than our baseline model. Let's now try another type of ensembling called stacking. With stacking, we use the outputs of each of our individual models as features into a new model. In this case, where we have a decision tree, a naive baayes classifier, and a svc classifier, these will be the three features that a new model predicts on. \n",
    "\n",
    "Let's try running these three through a Naive Bayes Classifier and see what the results look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2468f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T22:48:53.999965Z",
     "iopub.status.busy": "2022-11-14T22:48:53.999467Z",
     "iopub.status.idle": "2022-11-14T22:48:54.007533Z",
     "shell.execute_reply": "2022-11-14T22:48:54.005927Z",
     "shell.execute_reply.started": "2022-11-14T22:48:53.999924Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "ens_stack = StackingClassifier(estimators = [('dt', dt_voting), ('lr',lr_voting), ('nb',GaussianNB())], final_estimator = GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485be6d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T22:48:55.542640Z",
     "iopub.status.busy": "2022-11-14T22:48:55.542137Z",
     "iopub.status.idle": "2022-11-14T22:50:18.188631Z",
     "shell.execute_reply": "2022-11-14T22:50:18.187036Z",
     "shell.execute_reply.started": "2022-11-14T22:48:55.542603Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_accuracy = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "stack_f1 = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('stacking_accuracy: ' +str(stack_accuracy))\n",
    "print('stacking F1_Macro Score: '+str(stack_f1))\n",
    "print('stacking_accuracy_avg: ' + str(stack_accuracy.mean()) +'  |  stack_f1_avg: '+str(stack_f1.mean()))\n",
    "\n",
    "#in this case it didn't outperfrom, but it often does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944be96b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Models\n",
    "The last main type of ensemble approach that we see is one that is designed that way algorithmically. Typically, random forest or gradient boosted models have ensembling built into their implementation. Let's explor random forest and see how this approach works for our data. (We have a breakdown of the main ensembling techniques in our full course on algorithms). These algorithms leverage multiple decision trees to either vote or give pass information on to subsequent models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a770a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T22:51:47.334705Z",
     "iopub.status.busy": "2022-11-14T22:51:47.334146Z",
     "iopub.status.idle": "2022-11-14T23:00:08.874431Z",
     "shell.execute_reply": "2022-11-14T23:00:08.872939Z",
     "shell.execute_reply.started": "2022-11-14T22:51:47.334662Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#first let's try a non-tuned implementation \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_accuracy = cross_val_score(rf,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "rf_f1 = cross_val_score(rf,X_train,y_train.values.ravel(), cv=3, scoring ='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1246024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T23:00:08.877862Z",
     "iopub.status.busy": "2022-11-14T23:00:08.877001Z",
     "iopub.status.idle": "2022-11-14T23:00:08.888957Z",
     "shell.execute_reply": "2022-11-14T23:00:08.887040Z",
     "shell.execute_reply.started": "2022-11-14T23:00:08.877812Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('rf_accuracy: ' +str(rf_accuracy))\n",
    "print('rf F1_Macro Score: '+str(rf_f1))\n",
    "print('rf_accuracy_avg: ' + str(rf_accuracy.mean()) +'  |  rf_f1_avg: '+str(rf_f1.mean()))\n",
    "\n",
    "#of course, you can tune this model like the others! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b189196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T00:15:49.808888Z",
     "iopub.status.busy": "2022-11-15T00:15:49.808394Z",
     "iopub.status.idle": "2022-11-15T00:15:49.817105Z",
     "shell.execute_reply": "2022-11-15T00:15:49.815309Z",
     "shell.execute_reply.started": "2022-11-15T00:15:49.808849Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print([int(x) for x in np.linspace(10, 110, num = 11)]+[None])#.append(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf394a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T00:16:08.016392Z",
     "iopub.status.busy": "2022-11-15T00:16:08.015842Z",
     "iopub.status.idle": "2022-11-15T01:21:04.735934Z",
     "shell.execute_reply": "2022-11-15T01:21:04.733597Z",
     "shell.execute_reply.started": "2022-11-15T00:16:08.016349Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)]+[None],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}\n",
    "rs_rf = RandomizedSearchCV(estimator = rf, param_distributions =random_grid, n_iter =100, cv = 3, random_state = 42, scoring ='f1')\n",
    "\n",
    "rs_rf.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11154ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T00:06:50.501507Z",
     "iopub.status.busy": "2022-11-15T00:06:50.500975Z",
     "iopub.status.idle": "2022-11-15T00:09:36.325077Z",
     "shell.execute_reply": "2022-11-15T00:09:36.323829Z",
     "shell.execute_reply.started": "2022-11-15T00:06:50.501469Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nb.fit(X_train,y_train.values.ravel())\n",
    "ens.fit(X_train,y_train.values.ravel())\n",
    "dt_voting.fit(X_train,y_train.values.ravel())\n",
    "ens_stack.fit(X_train,y_train.values.ravel())\n",
    "rf_est = RandomForestClassifier()\n",
    "rf_est.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "nb_pred = nb.predict(X_test)\n",
    "ens_pred = ens.predict(X_test)\n",
    "dt_pred = dt_voting.predict(X_test)\n",
    "ens_stack_pred = ens_stack.predict(X_test)\n",
    "rf_pred = rf_est.predict(X_test)\n",
    "\n",
    "print('baseline score ' + str(f1_score(y_test,nb_pred)))\n",
    "print('dt score ' + str(f1_score(y_test,dt_pred)))\n",
    "print('voting score ' + str(f1_score(y_test,ens_pred)))\n",
    "print('Stacking score ' + str(f1_score(y_test,ens_stack_pred)))\n",
    "print('rf score ' + str(f1_score(y_test,rf_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346bcd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Summary\n",
    "In this notebook we implemented the following: \n",
    "We will cover:\n",
    "- Baseline creation\n",
    "- Model selection\n",
    "- Parameter tuning\n",
    "     - manual\n",
    "     - gridsearch\n",
    "     - random search\n",
    "     - basian optomization\n",
    "- Ensemble models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b239474",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.199826,
   "end_time": "2022-11-19T00:32:34.413282",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-19T00:32:25.213456",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
