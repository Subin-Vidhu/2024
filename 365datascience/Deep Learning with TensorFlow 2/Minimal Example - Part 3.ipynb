{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression. Minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random input data to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, we should declare a variable containing the size of the training set we want to generate.\n",
    "observations = 1000\n",
    "\n",
    "# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.\n",
    "# We have picked x and z, since it is easier to differentiate them.\n",
    "# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).\n",
    "# The size of xs and zs is observations x 1. In this case: 1000 x 1.\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "# Combine the two dimensions.сх of the input into one input matrix. \n",
    "# This is the X matrix from the linear model y = x*w + b.\n",
    "# column_stack is a Numpy method, which combines two matrices (vectors) into one.\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. \n",
    "# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.\n",
    "print (inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the targets we will aim at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.\n",
    "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Check the shape of the targets just in case.\n",
    "print (targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training data\n",
    "The point is to see that there is a strong trend that our model should learn to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.\n",
    "# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.\n",
    "targets = targets.reshape(observations,)\n",
    "\n",
    "# Plotting according to the conventional matplotlib.pyplot syntax\n",
    "\n",
    "# Declare the figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# A method allowing us to create the 3D plot\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Choose the axes.\n",
    "ax.plot(xs, zs, targets)\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('xs')\n",
    "ax.set_ylabel('zs')\n",
    "ax.set_zlabel('Targets')\n",
    "\n",
    "# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100\n",
    "# to azim = 0 ; azim = 200, or whatever. Check and see what happens.\n",
    "ax.view_init(azim=100)\n",
    "\n",
    "# So far we were just describing the plot. This method actually shows the plot. \n",
    "plt.show()\n",
    "\n",
    "# We reshape the targets back to the shape that they were in before plotting.\n",
    "# This reshaping is a side-effect of the 3D plot. Sorry for that.\n",
    "targets = targets.reshape(observations,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will initialize the weights and biases randomly in some small initial range.\n",
    "# init_range is the variable that will measure that.\n",
    "# You can play around with the initial range, but we don't really encourage you to do so.\n",
    "# High initial ranges may prevent the machine learning algorithm from learning.\n",
    "init_range = 0.1\n",
    "\n",
    "# Weights are of size k x m, where k is the number of input variables and m is the number of output variables\n",
    "# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "# Biases are of size 1 since there is only 1 output. The bias is a scalar.\n",
    "biases = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "\n",
    "#Print the weights to get a sense of how they were initialized.\n",
    "print (weights)\n",
    "print (biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set some small learning rate (denoted eta in the lecture). \n",
    "# 0.02 is going to work quite well for our example. Once again, you can play around with it.\n",
    "# It is HIGHLY recommended that you play around with it.\n",
    "learning_rate = 0.02"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
