{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "C4W3_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prTKL3d2kGZE"
      },
      "source": [
        "# Week 3: Variational Autoencoders on Anime Faces\n",
        "\n",
        "For this exercise, you will train a Variational Autoencoder (VAE) using the [anime faces dataset by MckInsey666](https://github.com/bchao1/Anime-Face-Dataset). \n",
        "\n",
        "You will train the model using the techniques discussed in class. At the end, you should save your model and download it from Colab so that it can be submitted to the autograder for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nms7__aqDj48"
      },
      "source": [
        "***Important:*** *This colab notebook has read-only access so you won't be able to save your changes. If you want to save your work periodically, please click `File -> Save a Copy in Drive` to create a copy in your account, then work from there.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qxq9uZAk3Lh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MooRFGEeI1zb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import random\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9rq-0uk7nS"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjhN6GgfmUfx"
      },
      "source": [
        "# set a random seed\n",
        "np.random.seed(51)\n",
        "\n",
        "# parameters for building the model and training\n",
        "BATCH_SIZE=2000\n",
        "LATENT_DIM=512\n",
        "IMAGE_SIZE=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXTdjxmolDBo"
      },
      "source": [
        "## Download the Dataset\n",
        "\n",
        "You will download the Anime Faces dataset and save it to a local directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxKW6Q88KHcL"
      },
      "source": [
        "# make the data directory\n",
        "try:\n",
        "  os.mkdir('/tmp/anime')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# download the zipped dataset to the data directory\n",
        "data_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Resources/anime-faces.zip\"\n",
        "data_file_name = \"animefaces.zip\"\n",
        "download_dir = '/tmp/anime/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "\n",
        "# extract the zip file\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6WCIlclWaA"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbaVpD18ggOX"
      },
      "source": [
        "Next is preparing the data for training and validation. We've provided you some utilities below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTlx97U_JDPB"
      },
      "source": [
        "# Data Preparation Utilities\n",
        "\n",
        "def get_dataset_slice_paths(image_dir):\n",
        "  '''returns a list of paths to the image files'''\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "def map_image(image_filename):\n",
        "  '''preprocesses the images'''\n",
        "  img_raw = tf.io.read_file(image_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "  image = image / 255.0  \n",
        "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uFon6vdhMhi"
      },
      "source": [
        "You will use the functions above to generate the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGoCJ6DPJHL8"
      },
      "source": [
        "# get the list containing the image paths\n",
        "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
        "\n",
        "# shuffle the paths\n",
        "random.shuffle(paths)\n",
        "\n",
        "# split the paths list into to training (80%) and validation sets(20%).\n",
        "paths_len = len(paths)\n",
        "train_paths_len = int(paths_len * 0.8)\n",
        "\n",
        "train_paths = paths[:train_paths_len]\n",
        "val_paths = paths[train_paths_len:]\n",
        "\n",
        "# load the training image paths into tensors, create batches and shuffle\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
        "training_dataset = training_dataset.map(map_image)\n",
        "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "# load the validation image paths into tensors and create batches\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
        "validation_dataset = validation_dataset.map(map_image)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(f'number of batches in the training set: {len(training_dataset)}')\n",
        "print(f'number of batches in the validation set: {len(validation_dataset)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZRga9vlonx"
      },
      "source": [
        "## Display Utilities\n",
        "\n",
        "We've also provided some utilities to help in visualizing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC1cpLViJLIu"
      },
      "source": [
        "def display_faces(dataset, size=9):\n",
        "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
        "  dataset = dataset.unbatch().take(size)\n",
        "  n_cols = 3\n",
        "  n_rows = size//n_cols + 1\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  i = 0\n",
        "  for image in dataset:\n",
        "    i += 1\n",
        "    disp_img = np.reshape(image, (64,64,3))\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(disp_img)\n",
        "\n",
        "\n",
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Displays a row of images.'''\n",
        "  for idx, image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    image = np.reshape(image, shape)\n",
        "    plt.imshow(image)\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_predicted):\n",
        "  '''Displays input and predicted images.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2brROh6qLJbs"
      },
      "source": [
        "Let's see some of the anime faces from the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eZsrZtqJOzv"
      },
      "source": [
        "display_faces(validation_dataset, size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSBtdCVim9aC"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQvzWaNqLrB1"
      },
      "source": [
        "You will be building your VAE in the following sections. Recall that this will follow and encoder-decoder architecture and can be summarized by the figure below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNxIUUS9ng9"
      },
      "source": [
        "### Sampling Class\n",
        "\n",
        "You will start with the custom layer to provide the Gaussian noise input along with the mean (mu) and standard deviation (sigma) of the encoder's output. Recall the equation to combine these:\n",
        "\n",
        "$$z = \\mu + e^{0.5\\sigma} * \\epsilon  $$\n",
        "\n",
        "where $\\mu$ = mean, $\\sigma$ = standard deviation, and $\\epsilon$ = random sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-3qk6ZBm0Fl"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Generates a random sample and combines with the encoder output\n",
        "    \n",
        "    Args:\n",
        "      inputs -- output tensor from the encoder\n",
        "\n",
        "    Returns:\n",
        "      `inputs` tensors combined with a random sample\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    mu, sigma = inputs\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "    ### END CODE HERE ###\n",
        "    return  z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjCSa7Y-Gvk"
      },
      "source": [
        "### Encoder Layers\n",
        "\n",
        "Next, please use the Functional API to stack the encoder layers and output `mu`, `sigma` and the shape of the features before flattening. We expect you to use 3 convolutional layers (instead of 2 in the ungraded lab) but feel free to revise as you see fit. Another hint is to use `1024` units in the Dense layer before you get mu and sigma (we used `20` for it in the ungraded lab).\n",
        "\n",
        "*Note: If you did Week 4 before Week 3, please do not use LeakyReLU activations yet for this particular assignment. The grader for Week 3 does not support LeakyReLU yet. This will be updated but for now, you can use `relu` and `sigmoid` just like in the ungraded lab.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VSVYjDim4Dk"
      },
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"Defines the encoder's layers.\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "\n",
        "  Returns:\n",
        "    mu -- learned mean\n",
        "    sigma -- learned standard deviation\n",
        "    batch_3.shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv3\")(x)\n",
        "  batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "  x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)\n",
        "  \n",
        "  \n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  # revise `batch_3.shape` here if you opted not to use 3 Conv2D layers\n",
        "  return mu, sigma, batch_3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOy7wPPY-g-N"
      },
      "source": [
        "### Encoder Model\n",
        "\n",
        "You will feed the output from the above function to the `Sampling layer` you defined earlier. That will have the latent representations that can be fed to the decoder network later. Please complete the function below to build the encoder network with the `Sampling` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Y-wLFym60N"
      },
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"Defines the encoder model with the Sampling layer\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    model -- the encoder model\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "  z = Sampling()((mu, sigma))\n",
        "  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "  ### END CODE HERE ###\n",
        "  model.summary()\n",
        "  return model, conv_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ENB-6a-0R5"
      },
      "source": [
        "### Decoder Layers\n",
        "\n",
        "Next, you will define the decoder layers. This will expand the latent representations back to the original image dimensions. After training your VAE model, you can use this decoder model to generate new data by feeding random inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlTjAzgsm9Vn"
      },
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "  \"\"\"Defines the decoder layers.\n",
        "  Args:\n",
        "    inputs -- output of the encoder \n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    tensor containing the decoded output\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  # reshape output using the conv_shape dimensions\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "\n",
        "  # upsample the features back to the original dimensions\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_1\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n",
        "  \n",
        "    \n",
        "  \n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLLz84r_MlN"
      },
      "source": [
        "### Decoder Model\n",
        "\n",
        "Please complete the function below to output the decoder model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUgTyqNFm_jR"
      },
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  \"\"\"Defines the decoder model.\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    model -- the decoder model\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  ### END CODE HERE ###\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps0yuE1d_cQc"
      },
      "source": [
        "### Kullback–Leibler Divergence\n",
        "\n",
        "Next, you will define the function to compute the [Kullback–Leibler Divergence](https://arxiv.org/abs/2002.07514) loss. This will be used to improve the generative capability of the model. This code is already given.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tngFmDDwnDn-"
      },
      "source": [
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    outputs -- output of the Sampling layer\n",
        "    mu -- mean\n",
        "    sigma -- standard deviation\n",
        "\n",
        "  Returns:\n",
        "    KLD loss\n",
        "  \"\"\"\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  return tf.reduce_mean(kl_loss) * -0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1I431I_og7"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "Please define the whole VAE model. Remember to use `model.add_loss()` to add the KL reconstruction loss. This will be accessed and added to the loss later in the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuPHg28JnGCp"
      },
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"Defines the VAE model\n",
        "  Args:\n",
        "    encoder -- the encoder model\n",
        "    decoder -- the decoder model\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    the complete VAE model\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "  reconstructed = decoder(z)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "  \n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lbWSKbALf-"
      },
      "source": [
        "Next, please define a helper function to return the encoder, decoder, and vae models you just defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnPo0Pr3nIts",
        "lines_to_next_cell": 2
      },
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "  vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "  \n",
        "  ### END CODE HERE ###\n",
        "  return encoder, decoder, vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJsdzZTPVgOn"
      },
      "source": [
        "Let's use the function above to get the models we need in the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdr3CUznL5Z"
      },
      "source": [
        "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IwN5vlAb5w"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "You will now configure the model for training. We defined some losses, the optimizer, and the loss metric below but you can experiment with others if you like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHPwSmZFnQ_2"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRzFxYkAvXH"
      },
      "source": [
        "You will generate 16 images in a 4x4 grid to show\n",
        "progress of image generation. We've defined a utility function for that below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGe445j0nTmf"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  \"\"\"Helper function to plot our 16 images\n",
        "\n",
        "  Args:\n",
        "\n",
        "  model -- the decoder model\n",
        "  epoch -- current epoch number during training\n",
        "  step -- current step number during training\n",
        "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
        "  \"\"\"\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i, :, :, :] * 255\n",
        "      img = img.astype('int32')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgJfazr6A_py"
      },
      "source": [
        "You can now start the training loop. You are asked to select the number of epochs and to complete the subection on updating the weights. The general steps are:\n",
        "\n",
        "* feed a training batch to the VAE model\n",
        "* compute the reconstruction loss (hint: use the **mse_loss** defined above instead of `bce_loss` in the ungraded lab, then multiply by the flattened dimensions of the image (i.e. 64 x 64 x 3)\n",
        "* add the KLD regularization loss to the total loss (you can access the `losses` property of the `vae` model)\n",
        "* get the gradients\n",
        "* use the optimizer to update the weights\n",
        "\n",
        "\n",
        "When training your VAE, you might notice that there’s not a lot of variation in the faces. But don’t let that deter you! We’ll test based on how well it does in reconstructing the original faces, and not how well it does in creating new faces.\n",
        "\n",
        "The training will also take a long time (more than 30 minutes) and that is to be expected. If you used the mean loss metric suggested above, train the model until that is down to around 320 before submitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvL1bHXJnajM"
      },
      "source": [
        "# Training loop. Display generated images each epoch\n",
        "\n",
        "### START CODE HERE ###\n",
        "epochs = 50\n",
        "### END CODE HERE ###\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(training_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      ### START CODE HERE ### \n",
        "      reconstructed = vae(x_batch_train)\n",
        "      # Compute reconstruction loss\n",
        "      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "      loss = bce_loss(flattened_inputs, flattened_outputs) * 784\n",
        "      loss += sum(vae.losses)        \n",
        "\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      display.clear_output(wait=False)    \n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5wfzGfABny6"
      },
      "source": [
        "# Plot Reconstructed Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnQQlWZHaj90"
      },
      "source": [
        "As mentioned, your model will be graded on how well it is able to reconstruct images (not generate new ones). You can get a glimpse of how it is doing with the code block below. It feeds in a batch from the test set and plots a row of input (top) and output (bottom) images. Don't worry if the outputs are a blurry. It will look something like below:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OPMbZOxX9fx8tK6CGVbrMaQdgyOiQJIC\" width=\"75%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfIbqTIKSXEe"
      },
      "source": [
        "test_dataset = validation_dataset.take(1)\n",
        "output_samples = []\n",
        "\n",
        "for input_image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "idxs = np.random.choice(64, size=10)\n",
        "\n",
        "vae_predicted = vae.predict(test_dataset)\n",
        "display_results(output_samples[idxs], vae_predicted[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YKUOCA5BtAA"
      },
      "source": [
        "# Plot Generated Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylxL9z15ctsy"
      },
      "source": [
        "Using the default parameters, it can take a long time to train your model well enough to generate good fake anime faces. In case you decide to experiment, we provided the code block below to display an 8x8 gallery of fake data generated from your model. Here is a sample gallery generated after 50 epochs.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QwElgfg5TY6vCgI1FK6vdI8Bo6UZKfuX\" width=\"75%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCpTybvGSS6L"
      },
      "source": [
        "def plot_images(rows, cols, images, title):\n",
        "    '''Displays images in a grid.'''\n",
        "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
        "\n",
        "    plt.figure(figsize=(12,12))       \n",
        "    plt.imshow(grid)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# initialize random inputs\n",
        "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
        "\n",
        "# get predictions from the decoder model\n",
        "predictions= decoder.predict(test_vector_for_generation)\n",
        "\n",
        "# plot the predictions\n",
        "plot_images(8,8,predictions,'Generated Images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4IixoasCfoR"
      },
      "source": [
        "### Save the Model\n",
        "\n",
        "Once your satisfied with the results, please save and download the model. Afterwards, please go back to the Coursera submission portal to upload your h5 file to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9E8qwDAVMPs"
      },
      "source": [
        "vae.save(\"anime.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCDqKNyWJ14I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}