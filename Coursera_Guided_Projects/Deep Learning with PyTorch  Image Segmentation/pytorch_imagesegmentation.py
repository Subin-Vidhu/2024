# -*- coding: utf-8 -*-
"""PyTorch-ImageSegmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EiCnl6FlAqUoW8sZ9AgGISpoy4uAXcS7

# Task 1 : Set up colab gpu runtime environment
"""
'''
!pip install segmentation-models-pytorch
!pip install -U git+https://github.com/albumentations-team/albumentations
!pip install --upgrade opencv-contrib-python
'''
"""# Download Dataset

original author of the dataset :
https://github.com/VikramShenoy97/Human-Segmentation-Dataset

"""
'''
!git clone https://github.com/parth1620/Human-Segmentation-Dataset-master.git
'''
# Some Common Imports
"""

import sys
sys.path.append('/content/Human-Segmentation-Dataset-master')

import torch
import cv2

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from tqdm import tqdm

import helper

"""# Task : 2 Setup Configurations"""

CSV_FILE = '/content/Human-Segmentation-Dataset-master/train.csv'
DATA_DIR = '/content/'
DEVICE = 'cuda'

EPOCHS = 100
BATCH_SIZE = 16
IMAGE_SIZE = 320
LEARNING_RATE = 0.003

ENCODER = 'timm-efficientnet-b0'
WEIGHTS = 'imagenet'

df = pd.read_csv(CSV_FILE)
df.head()

row = df.loc[4]

image_path = DATA_DIR + row.images
mask_path = DATA_DIR + row.masks

image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) / 255.0

print(f"Image Shape : {image.shape}")
print(f"Mask Shape : {mask.shape}")

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))

ax1.set_title('IMAGE')
ax1.imshow(image)

ax2.set_title('GROUND TRUTH')
ax2.imshow(mask,cmap = 'gray')

train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)

"""# Task 3 : Augmentation Functions

albumentation documentation : https://albumentations.ai/docs/
"""

import albumentations as A

from albumentations import Compose, OneOf, HorizontalFlip, VerticalFlip, Rotate

def get_train_augs():
    return Compose([
        HorizontalFlip(p=0.5),
        VerticalFlip(p=0.5),
        Rotate(limit=45, p=0.5),
        # Try removing or adjusting the RandomCrop operation
        # RandomCrop(height=256, width=256, p=0.5),
    ])

def get_valid_augs():
    return Compose([
        # No augmentations for validation
    ])



"""# Task 4 : Create Custom Dataset"""

from torch.utils.data import Dataset, DataLoader

from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
import torch

class SegmentationDataset(Dataset):
  def __init__(self, df, augmentations):
    self.df = df
    self.augmentations = augmentations

  def __len__(self):
    return len(self.df)

  def __getitem__(self, idx):
    row = self.df.iloc[idx]
    image_path = DATA_DIR + row.images
    mask_path = DATA_DIR + row.masks
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # Ensure the mask has the same dimensions as the image
    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))

    mask = np.expand_dims(mask, axis=-1)
    if self.augmentations:
      aug = self.augmentations(image=image, mask=mask)
      image = aug['image']
      mask = aug['mask']

    image = np.transpose(image, (2, 0, 1)).astype(np.float32)
    mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)
    image = torch.Tensor(image) / 255.0
    mask = torch.round(torch.Tensor(mask) / 255.0)
    return image, mask

trainset = SegmentationDataset(train_df, get_train_augs())
validset = SegmentationDataset(valid_df, get_valid_augs())

print(f"Size of Trainset : {len(trainset)}")
print(f"Size of Validset : {len(validset)}")

idx = 32
image, mask = trainset[idx]
helper.show_image(image, mask)

"""# Task 5 : Load dataset into batches"""

from torch.utils.data import DataLoader

trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
validloader = DataLoader(validset, batch_size=BATCH_SIZE)

print(f"Total number of batches in trainloader : {len(trainloader)}") #232/16~15, 232 is the length, and 16 is the given batch_size
print(f"Total number of batches in validloader : {len(validloader)}")

for image,mask in trainloader:
  print(f"Image batch shape : {image.shape}")
  print(f"Mask batch shape : {mask.shape}")
  break

"""# Task 6 : Create Segmentation Model

segmentation_models_pytorch documentation : https://smp.readthedocs.io/en/latest/
"""

from torch import nn
import segmentation_models_pytorch as smp
from segmentation_models_pytorch.losses import DiceLoss

class SegmentationModel(nn.Module):
  def __init__(self):
    super(SegmentationModel, self).__init__()

    self.arc = smp.Unet(
        encoder_name=ENCODER,
        encoder_weights=WEIGHTS,
        in_channels=3,
        classes=1,
        activation=None
    )

    self.loss_fn = DiceLoss(mode='binary')

  def forward(self, images, masks):
        logits = self.arc(images)
        if masks != None:
            loss1 = self.loss_fn(logits, masks)
            loss2 = torch.nn.BCEWithLogitsLoss()(logits, masks) # Use torch.nn directly
            loss = loss1 + loss2
            return logits, loss
        return logits

model = SegmentationModel().to(DEVICE)

"""# Task 7 : Create Train and Validation Function"""

def train_fn(data_loader, model, optimizer):
  model.train()
  total_loss = 0

  for images, masks in tqdm(data_loader):
    images = images.to(DEVICE)
    masks = masks.to(DEVICE)
    optimizer.zero_grad()
    logits, loss = model(images, masks)
    loss.backward()
    optimizer.step()
    total_loss += loss.item()
  return total_loss/len(data_loader) #avg loss

def eval_fn(data_loader, model):
  model.eval()
  total_loss = 0

  with torch.no_grad():
    for images, masks in tqdm(data_loader):
      images = images.to(DEVICE)
      masks = masks.to(DEVICE)
      logits, loss = model(images, masks)
      total_loss += loss.item()
  return total_loss/len(data_loader)

"""# Task 8 : Train Model"""

optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

best_valid_loss = np.inf
for epoch in range(EPOCHS):
  train_loss = train_fn(trainloader, model, optimizer)
  valid_loss = eval_fn(validloader, model)
  print(f"Epoch : {epoch+1}")
  print(f"Train Loss : {train_loss:.4f}")
  print(f"Valid Loss : {valid_loss:.4f}")
  if valid_loss < best_valid_loss:
    best_valid_loss = valid_loss
    torch.save(model.state_dict(), 'best_model.pth')
    print('Saved Best Model!')

"""# Task 9 : Inference"""

idx = np.random.randint(0, len(validset))
idx = 20
image, mask = validset[idx]

model.load_state_dict(torch.load('/content/best_model.pth'))

logits_mask = model(image.to(DEVICE).unsqueeze(0), masks=None) # Add masks=None as an argument.
pred_mask = torch.sigmoid(logits_mask)
pred_mask = (pred_mask > 0.5) * 1.0
pred_mask = pred_mask.detach().cpu().squeeze(0)

helper.show_image(image, mask, pred_mask)

#!pip install torchmetrics

from torchmetrics import Dice, JaccardIndex
def calculate_metrics(pred_mask, true_mask):
    dice_metric = Dice(average='micro').to(DEVICE)
    iou_metric = JaccardIndex(task='binary', num_classes=1).to(DEVICE)

    dice_score = dice_metric(pred_mask, true_mask)
    iou_score = iou_metric(pred_mask, true_mask)

    return dice_score.item(), iou_score.item()

def inference_with_metrics(model, data_loader):
    model.eval()
    dice_scores = []
    iou_scores = []

    with torch.no_grad():
        for images, masks in tqdm(data_loader):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE).long() # Convert mask to long tensor

            logits = model(images, masks=None)
            pred_masks = (torch.sigmoid(logits) > 0.5).float()

            dice_score, iou_score = calculate_metrics(pred_masks, masks)
            dice_scores.append(dice_score)
            iou_scores.append(iou_score)

    avg_dice = sum(dice_scores) / len(dice_scores)
    avg_iou = sum(iou_scores) / len(iou_scores)
    return avg_dice, avg_iou

# After training, load the best model and calculate metrics
model.load_state_dict(torch.load('best_model.pth'))
model.to(DEVICE)

print("Calculating metrics on validation set...")
val_dice, val_iou = inference_with_metrics(model, validloader)
print(f"Validation Dice Score: {val_dice:.4f}")
print(f"Validation IoU Score: {val_iou:.4f}")

# Optional: Calculate metrics on training set as well
print("Calculating metrics on training set...")
train_dice, train_iou = inference_with_metrics(model, trainloader)
print(f"Training Dice Score: {train_dice:.4f}")
print(f"Training IoU Score: {train_iou:.4f}")

from torchmetrics import Dice, JaccardIndex
from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryF1Score

# ... [SegmentationModel class and other functions remain the same] ...

def calculate_metrics(pred_mask, true_mask):
    # Use 'micro' for overall Dice score or 'macro' for average per-class Dice
    dice_metric = Dice(average='micro', num_classes=2).to(DEVICE)
    iou_metric = JaccardIndex(task='binary', num_classes=2).to(DEVICE)
    precision_metric = BinaryPrecision().to(DEVICE)
    recall_metric = BinaryRecall().to(DEVICE)
    f1_metric = BinaryF1Score().to(DEVICE)

    dice_score = dice_metric(pred_mask, true_mask)
    iou_score = iou_metric(pred_mask, true_mask)
    precision = precision_metric(pred_mask, true_mask)
    recall = recall_metric(pred_mask, true_mask)
    f1 = f1_metric(pred_mask, true_mask)

    return {
        'dice': dice_score.tolist(),
        'iou': iou_score.tolist(),
        'precision': precision.item(),
        'recall': recall.item(),
        'f1': f1.item()
    }

def inference_with_metrics(model, data_loader):
    model.eval()
    all_metrics = []

    with torch.no_grad():
        for images, masks in tqdm(data_loader):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE).long()  # Convert mask to long tensor

            logits = model(images, masks=None)
            pred_masks = (torch.sigmoid(logits) > 0.5).float()

            metrics = calculate_metrics(pred_masks, masks)
            all_metrics.append(metrics)

    # Calculate average metrics
    avg_metrics = {
        'dice': [sum(m['dice'][i] for m in all_metrics) / len(all_metrics) for i in range(2)],
        'iou': [sum(m['iou'][i] for m in all_metrics) / len(all_metrics) for i in range(2)],
        'precision': sum(m['precision'] for m in all_metrics) / len(all_metrics),
        'recall': sum(m['recall'] for m in all_metrics) / len(all_metrics),
        'f1': sum(m['f1'] for m in all_metrics) / len(all_metrics)
    }

    return avg_metrics

# After training, load the best model and calculate metrics
model.load_state_dict(torch.load('best_model.pth'))
model.to(DEVICE)

print("Calculating metrics on validation set...")
val_metrics = inference_with_metrics(model, validloader)
print("Validation Metrics:")
print(f"Dice Score: Background: {val_metrics['dice'][0]:.4f}, Foreground: {val_metrics['dice'][1]:.4f}")
print(f"IoU Score: Background: {val_metrics['iou'][0]:.4f}, Foreground: {val_metrics['iou'][1]:.4f}")
print(f"Precision: {val_metrics['precision']:.4f}")
print(f"Recall: {val_metrics['recall']:.4f}")
print(f"F1 Score: {val_metrics['f1']:.4f}")

# Optional: Calculate metrics on training set as well
print("\nCalculating metrics on training set...")
train_metrics = inference_with_metrics(model, trainloader)
print("Training Metrics:")
print(f"Dice Score: Background: {train_metrics['dice'][0]:.4f}, Foreground: {train_metrics['dice'][1]:.4f}")
print(f"IoU Score: Background: {train_metrics['iou'][0]:.4f}, Foreground: {train_metrics['iou'][1]:.4f}")
print(f"Precision: {train_metrics['precision']:.4f}")
print(f"Recall: {train_metrics['recall']:.4f}")
print(f"F1 Score: {train_metrics['f1']:.4f}")



from torchmetrics import Dice, JaccardIndex
from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryF1Score

# ... [SegmentationModel class and other functions remain the same] ...

def calculate_metrics(pred_mask, true_mask):
    # Use 'micro' for overall Dice score or 'macro' for average per-class Dice
    dice_metric = Dice(average='micro').to(DEVICE) # num_classes should be removed for binary segmentation
    iou_metric = JaccardIndex(task='binary').to(DEVICE) # num_classes should be removed for binary segmentation
    precision_metric = BinaryPrecision().to(DEVICE)
    recall_metric = BinaryRecall().to(DEVICE)
    f1_metric = BinaryF1Score().to(DEVICE)

    dice_score = dice_metric(pred_mask, true_mask)
    iou_score = iou_metric(pred_mask, true_mask)
    precision = precision_metric(pred_mask, true_mask)
    recall = recall_metric(pred_mask, true_mask)
    f1 = f1_metric(pred_mask, true_mask)

    return {
        'dice': dice_score.tolist(),
        'iou': iou_score.tolist(),
        'precision': precision.item(),
        'recall': recall.item(),
        'f1': f1.item()
    }

def inference_with_metrics(model, data_loader):
    model.eval()
    all_metrics = []

    with torch.no_grad():
        for images, masks in tqdm(data_loader):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE).long()  # Convert mask to long tensor

            logits = model(images, masks=None)
            pred_masks = (torch.sigmoid(logits) > 0.5).float()

            metrics = calculate_metrics(pred_masks, masks)
            all_metrics.append(metrics)

    # Calculate average metrics
    avg_metrics = {
        'dice': sum(m['dice'] for m in all_metrics) / len(all_metrics), # removed indexing since dice is now a single value
        'iou': sum(m['iou'] for m in all_metrics) / len(all_metrics), # removed indexing since iou is now a single value
        'precision': sum(m['precision'] for m in all_metrics) / len(all_metrics),
        'recall': sum(m['recall'] for m in all_metrics) / len(all_metrics),
        'f1': sum(m['f1'] for m in all_metrics) / len(all_metrics)
    }

    return avg_metrics

# After training, load the best model and calculate metrics
model.load_state_dict(torch.load('best_model.pth'))
model.to(DEVICE)

print("Calculating metrics on validation set...")
val_metrics = inference_with_metrics(model, validloader)
print("Validation Metrics:")
print(f"Dice Score: Background: {val_metrics['dice'][0]:.4f}, Foreground: {val_metrics['dice'][1]:.4f}")
print(f"IoU Score: Background: {val_metrics['iou'][0]:.4f}, Foreground: {val_metrics['iou'][1]:.4f}")
print(f"Precision: {val_metrics['precision']:.4f}")
print(f"Recall: {val_metrics['recall']:.4f}")
print(f"F1 Score: {val_metrics['f1']:.4f}")

# Optional: Calculate metrics on training set as well
print("\nCalculating metrics on training set...")
train_metrics = inference_with_metrics(model, trainloader)
print("Training Metrics:")
print(f"Dice Score: Background: {train_metrics['dice'][0]:.4f}, Foreground: {train_metrics['dice'][1]:.4f}")
print(f"IoU Score: Background: {train_metrics['iou'][0]:.4f}, Foreground: {train_metrics['iou'][1]:.4f}")
print(f"Precision: {train_metrics['precision']:.4f}")
print(f"Recall: {train_metrics['recall']:.4f}")
print(f"F1 Score: {train_metrics['f1']:.4f}")



# After training, load the best model and calculate metrics
model.load_state_dict(torch.load('best_model.pth'))
model.to(DEVICE)

print("Calculating metrics on validation set...")
val_metrics = inference_with_metrics(model, validloader)
print("Validation Metrics:")
print(f"Dice Score: {val_metrics['dice']:.4f}") # Changed to output a single dice score
print(f"IoU Score: {val_metrics['iou']:.4f}") # Changed to output a single iou score
print(f"Precision: {val_metrics['precision']:.4f}")
print(f"Recall: {val_metrics['recall']:.4f}")
print(f"F1 Score: {val_metrics['f1']:.4f}")

# Optional: Calculate metrics on training set as well
print("\nCalculating metrics on training set...")
train_metrics = inference_with_metrics(model, trainloader)
print("Training Metrics:")
print(f"Dice Score: {train_metrics['dice']:.4f}") # Changed to output a single dice score
print(f"IoU Score: {train_metrics['iou']:.4f}") # Changed to output a single iou score
print(f"Precision: {train_metrics['precision']:.4f}")
print(f"Recall: {train_metrics['recall']:.4f}")
print(f"F1 Score: {train_metrics['f1']:.4f}")